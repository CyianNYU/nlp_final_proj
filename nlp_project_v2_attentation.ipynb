{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_load = 50000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "\n",
    "with open('cc.zh.300.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+3, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    ordered_words_ft.extend(['<pad>', '<unk>', '<s>'])\n",
    "    loaded_embeddings_ft[0,:] = np.zeros(300)\n",
    "    loaded_embeddings_ft[1,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft[2,:] = np.random.normal(size = 300)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+3, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+3\n",
    "        idx2words_ft[i+3] = s[0]\n",
    "        ordered_words_ft.append(s[0])\n",
    "    words_ft['<pad>'] = PAD_IDX\n",
    "    words_ft['<unk>'] = UNK_IDX\n",
    "    words_ft['<s>'] = SOS_IDX\n",
    "    idx2words_ft[PAD_IDX] = '<pad>'\n",
    "    idx2words_ft[UNK_IDX] = '<unk>'\n",
    "    idx2words_ft[SOS_IDX] = '<s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English embedding\n",
    "with open('wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft_en = np.zeros((words_to_load+4, 300))\n",
    "    words_ft_en = {}\n",
    "    idx2words_ft_en = {}\n",
    "    ordered_words_ft_en = []\n",
    "    ordered_words_ft_en.extend(['<pad>', '<unk>', '<s>', '</s>'])\n",
    "    loaded_embeddings_ft_en[0,:] = np.zeros(300)\n",
    "    loaded_embeddings_ft_en[1,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en[2,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en[3,:] = np.random.normal(size = 300)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft_en[i+4, :] = np.asarray(s[1:])\n",
    "        words_ft_en[s[0]] = i+4\n",
    "        idx2words_ft_en[i+4] = s[0]\n",
    "        ordered_words_ft_en.append(s[0])\n",
    "    words_ft_en['<pad>'] = PAD_IDX\n",
    "    words_ft_en['<unk>'] = UNK_IDX\n",
    "    words_ft_en['<s>'] = SOS_IDX\n",
    "    words_ft_en['</s>'] = EOS_IDX\n",
    "    idx2words_ft_en[PAD_IDX] = '<pad>'\n",
    "    idx2words_ft_en[UNK_IDX] = '<unk>'\n",
    "    idx2words_ft_en[SOS_IDX] = '<s>'\n",
    "    idx2words_ft_en[EOS_IDX] = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in chinese-english pairs\n",
    "lines_zh = open('iwslt-zh-en/train.tok.zh',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en = open('iwslt-zh-en/train.tok.en',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_zh_test = open('iwslt-zh-en/test.tok.zh',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_test = open('iwslt-zh-en/test.tok.en',encoding = 'utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add sos and eos in each sentence\n",
    "def add_sos_eos(lines):\n",
    "    \n",
    "    train = []\n",
    "    for l in lines:\n",
    "        l = '<s> ' + l + '</s>'\n",
    "        train.append(l)\n",
    "    return train\n",
    "zh_train = add_sos_eos(lines_zh)    \n",
    "en_train = add_sos_eos(lines_en)\n",
    "zh_test = add_sos_eos(lines_zh_test)\n",
    "en_test = add_sos_eos(lines_en_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data,eng = False):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = []\n",
    "        for token in tokens.split():\n",
    "            if eng == False:\n",
    "                try:\n",
    "                    index_list.append(words_ft[token])\n",
    "                except KeyError:\n",
    "                    index_list.append(UNK_IDX)\n",
    "            else:\n",
    "                try:\n",
    "                    index_list.append(words_ft_en[token])\n",
    "                except KeyError:\n",
    "                    index_list.append(UNK_IDX)\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_train_indices = token2index_dataset(zh_train)\n",
    "en_train_indices = token2index_dataset(en_train,eng = True)\n",
    "zh_test_indices = token2index_dataset(zh_test)\n",
    "en_test_indices = token2index_dataset(en_test,eng = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_sentence_length\n",
    "length_of_en = [len(x) for x in en_train]\n",
    "max_sentence_length_en = sorted(length_of_en)[-int(len(length_of_en)*0.01)]\n",
    "length_of_zh = [len(x) for x in zh_train]\n",
    "max_sentence_length_zh = sorted(length_of_zh)[-int(len(length_of_zh)*0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Data Loader\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class load_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list_s1,data_list_s2):\n",
    "        \"\"\"\n",
    "        @param data_list_zh: list of Chinese tokens \n",
    "        @param data_list_en: list of English tokens as TARGETS\n",
    "        \"\"\"\n",
    "        self.data_list_s1 = data_list_s1\n",
    "        self.data_list_s2 = data_list_s2\n",
    "        \n",
    "        assert (len(self.data_list_s1) == len(self.data_list_s2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list_s1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx_s1 = self.data_list_s1[key][:max_sentence_length_zh]\n",
    "        token_idx_s2 = self.data_list_s2[key][:max_sentence_length_en]\n",
    "        return [token_idx_s1, token_idx_s2, len(token_idx_s1), len(token_idx_s2)]\n",
    "\n",
    "def collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list_s1 = []\n",
    "    data_list_s2 = []\n",
    "    length_list_s1 = []\n",
    "    length_list_s2 = []\n",
    "    for datum in batch:\n",
    "        length_list_s1.append(datum[2])\n",
    "        length_list_s2.append(datum[3])\n",
    "        padded_vec_zh = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,max_sentence_length_zh-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_en = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,max_sentence_length_en-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_s1.append(padded_vec_zh[:max_sentence_length_zh])\n",
    "        data_list_s2.append(padded_vec_en[:max_sentence_length_en])\n",
    "    #print(type(data_list_s1[0]))\n",
    "    if torch.cuda.is_available and torch.has_cudnn:\n",
    "        return [torch.from_numpy(np.array(data_list_s1)).cuda(), torch.from_numpy(np.array(data_list_s2)).cuda(),\n",
    "                torch.LongTensor(length_list_s1).cuda(), torch.LongTensor(length_list_s2).cuda()]\n",
    "    else:    \n",
    "        return [torch.from_numpy(np.array(data_list_s1)), torch.from_numpy(np.array(data_list_s2)),\n",
    "                torch.LongTensor(length_list_s1), torch.LongTensor(length_list_s2)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 300 # fixed as from the input embedding data\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(zh_train_indices, en_train_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = load_dataset(zh_test_indices, en_test_indices)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_size, embed= torch.from_numpy(loaded_embeddings_ft).float(),num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers \n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed, freeze=True)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_size,num_layers=num_layers,batch_first=True)\n",
    "\n",
    "    def forward(self, data, hidden):\n",
    "        \n",
    "        batch_size, seq_len = data.size()\n",
    "        \n",
    "        embed = self.embedding(data)\n",
    "        \n",
    "        output, hidden = self.gru(embed,hidden)\n",
    "        #hidden = [n layers * n directions =1 , batch_size, hidden_size ]\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    # initialize the hidden with random numbers\n",
    "    def initHidden(self,batch_size):\n",
    "        return torch.randn(self.num_layers, batch_size, self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,emb_dim,hidden_size, output_size, embed= torch.from_numpy(loaded_embeddings_ft_en).float(),num_layers=1,\n",
    "                 dropout_p=0.1, max_length=max_sentence_length_zh):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers \n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed, freeze=True)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.gru = nn.GRU(emb_dim, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, data, hidden,encoder_outputs):\n",
    "        \n",
    "        ### embed: [1 * batch size * emb_dim = 300 ] ###\n",
    "        ### hidden: [1 * batch size * hidden_size = 300 ] ###\n",
    "        ### encoder_outputs: [batch size * max_sentence_length_zh * hidden_size = 300 ] ###\n",
    "        ### 因为这里concat之后，attn layer 他给的是 hidden size *2 \n",
    "        ### 所以我这儿的hidden size就只能写300了 \n",
    "        \n",
    "        embed = self.embedding(data)\n",
    "        embed = self.dropout(embed)    \n",
    "        ### torch.cat((embed, hidden), 2)  \n",
    "        ### [1 * batch size * (emb_dim + hidden_size) ]\n",
    "        \n",
    "        ### attn_weights: [1 * batch size * max_sentence_length_zh ]###\n",
    "        ### attn_weights[0].unsqueeze(1): [batch size * 1 * max_sentence_length_zh ]###\n",
    "        \n",
    "        ### softmax dim=2 因为最后一个dimension是 词组什么的，不能是1，1的话就是\n",
    "        ### 不同batch间这样比较了？\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embed, hidden), 2)), dim=2)\n",
    "        \n",
    "\n",
    "        ### torch.bmm(attn_weights[0].unsqueeze(1),encoder_outputs).squeeze(1) :\n",
    "        ### [batch size * 1 * hidden_size ]###\n",
    "\n",
    "        ### attn_applied: [batch size * hidden_size (= 300) ] ###\n",
    "     \n",
    "        attn_applied = torch.bmm(attn_weights[0].unsqueeze(1),\n",
    "                                 encoder_outputs).squeeze(1)\n",
    "        \n",
    "        ### output: [batch size * hidden_size (= 300) ] ###\n",
    "        ### embed[0]: [batch size * hidden_size (= 300) ] ###\n",
    "\n",
    "        output = torch.cat((embed[0], attn_applied), 1)\n",
    " \n",
    "        ### output: [1 * batch size * hidden_size (= 300) ] ###\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        ### output: [1 * batch size * hidden_size (= 300) ] ###\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return torch.randn(self.num_layers, batch_size, self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "#input_tensor: list of sentence tensor\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "          criterion):\n",
    "    \n",
    "    ### target_tensor [batch size, max_sentence_length_en = 377] ###\n",
    "    ### target_tensor [batch size, max_sentence_length_zh = 220] ###\n",
    "    batch_size_1, input_length = input_tensor.size()\n",
    "    batch_size_2, target_length = target_tensor.size()\n",
    "    \n",
    "    \n",
    "    encoder_hidden = encoder.initHidden(batch_size_1)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    ### encoder_hidden: 1 * batch * hidden size ### \n",
    "    ### encoder_output: batch size * max_sentence_length_zh * hidden size ### \n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor(np.array([[SOS_IDX]]*batch_size_1).reshape(1,batch_size_1),device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    #print(use_teacher_forcing)\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            \n",
    "            ### decoder_output: [batchsize,5000] ###\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden,encoder_output)\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            decoder_input = target_tensor[:,di].unsqueeze(0)  # Teacher forcing\n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden,encoder_output)\n",
    "                        \n",
    "            ### decoder_output [batch size, 50003]  ###\n",
    "            \n",
    "            ### topi is a [batch size, 1] tensor first we remove the size 1\n",
    "            ### demension then we add it at the beginning using squeeze\n",
    "            ### 有点脑残诶，做个转置不就好了？\n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            \n",
    "            ### decoder_input [1, batch size]  ###\n",
    "            decoder_input = decoder_input.unsqueeze(0)\n",
    " \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for i, (data_s1, data_s2, lengths_s1, lengths_s2) in enumerate(train_loader):\n",
    "            input_tensor = data_s1\n",
    "            target_tensor = data_s2\n",
    "            #print(\"train\",target_tensor.size())\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                             iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            if i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "    \n",
    "    \n",
    "    return plot_losses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 1s (- 0m 5s) (1 20%) 0.2166\n",
      "0m 56s (- 3m 47s) (1 20%) 1.7066\n",
      "1m 51s (- 7m 26s) (1 20%) 0.4284\n",
      "2m 46s (- 11m 5s) (1 20%) 0.4086\n",
      "3m 40s (- 14m 42s) (1 20%) 0.3917\n",
      "4m 35s (- 18m 22s) (1 20%) 0.3963\n",
      "5m 30s (- 22m 3s) (1 20%) 0.3883\n",
      "6m 26s (- 25m 46s) (1 20%) 0.3680\n",
      "7m 21s (- 29m 27s) (1 20%) 0.3868\n",
      "8m 17s (- 33m 9s) (1 20%) 0.3874\n",
      "9m 12s (- 36m 50s) (1 20%) 0.3746\n",
      "10m 8s (- 40m 32s) (1 20%) 0.3839\n",
      "11m 3s (- 44m 13s) (1 20%) 0.3615\n",
      "11m 58s (- 47m 55s) (1 20%) 0.3803\n",
      "12m 53s (- 51m 35s) (1 20%) 0.3687\n",
      "13m 49s (- 55m 17s) (1 20%) 0.3582\n",
      "14m 44s (- 58m 58s) (1 20%) 0.3580\n",
      "15m 40s (- 62m 41s) (1 20%) 0.3641\n",
      "16m 36s (- 66m 26s) (1 20%) 0.3580\n",
      "17m 32s (- 70m 9s) (1 20%) 0.3594\n",
      "18m 27s (- 73m 48s) (1 20%) 0.3775\n",
      "19m 21s (- 77m 27s) (1 20%) 0.3724\n",
      "20m 16s (- 81m 5s) (1 20%) 0.3602\n",
      "21m 11s (- 84m 44s) (1 20%) 0.3671\n",
      "22m 5s (- 88m 23s) (1 20%) 0.3590\n",
      "23m 0s (- 92m 2s) (1 20%) 0.3504\n",
      "23m 55s (- 95m 41s) (1 20%) 0.3784\n",
      "24m 50s (- 99m 21s) (1 20%) 0.3526\n",
      "25m 45s (- 103m 0s) (1 20%) 0.3687\n",
      "26m 40s (- 106m 40s) (1 20%) 0.3763\n",
      "27m 35s (- 110m 21s) (1 20%) 0.3651\n",
      "28m 30s (- 114m 3s) (1 20%) 0.3600\n",
      "29m 26s (- 117m 44s) (1 20%) 0.3513\n",
      "30m 21s (- 121m 26s) (1 20%) 0.3623\n",
      "31m 16s (- 125m 6s) (1 20%) 0.3503\n",
      "32m 12s (- 128m 48s) (1 20%) 0.3445\n",
      "33m 7s (- 132m 29s) (1 20%) 0.3506\n",
      "34m 2s (- 136m 11s) (1 20%) 0.3680\n",
      "34m 58s (- 139m 52s) (1 20%) 0.3511\n",
      "35m 53s (- 143m 34s) (1 20%) 0.3500\n",
      "36m 49s (- 147m 16s) (1 20%) 0.3615\n",
      "37m 44s (- 150m 58s) (1 20%) 0.3518\n",
      "38m 40s (- 154m 41s) (1 20%) 0.3368\n",
      "39m 36s (- 158m 26s) (1 20%) 0.3656\n",
      "40m 31s (- 162m 6s) (1 20%) 0.3489\n",
      "41m 26s (- 165m 45s) (1 20%) 0.3561\n",
      "42m 21s (- 169m 24s) (1 20%) 0.3304\n",
      "43m 16s (- 173m 4s) (1 20%) 0.3542\n",
      "44m 10s (- 176m 43s) (1 20%) 0.3325\n",
      "45m 5s (- 180m 23s) (1 20%) 0.3541\n",
      "46m 0s (- 184m 3s) (1 20%) 0.3409\n",
      "46m 56s (- 187m 44s) (1 20%) 0.3254\n",
      "47m 51s (- 191m 24s) (1 20%) 0.3312\n",
      "48m 46s (- 195m 5s) (1 20%) 0.3336\n",
      "49m 41s (- 198m 45s) (1 20%) 0.3352\n",
      "50m 36s (- 202m 26s) (1 20%) 0.3273\n",
      "51m 32s (- 206m 8s) (1 20%) 0.3512\n",
      "52m 27s (- 209m 49s) (1 20%) 0.3104\n",
      "53m 22s (- 213m 31s) (1 20%) 0.3283\n",
      "54m 18s (- 217m 13s) (1 20%) 0.3398\n",
      "55m 13s (- 220m 55s) (1 20%) 0.3338\n",
      "56m 9s (- 224m 37s) (1 20%) 0.3354\n",
      "57m 4s (- 228m 19s) (1 20%) 0.3486\n",
      "58m 0s (- 232m 1s) (1 20%) 0.3301\n",
      "58m 55s (- 235m 43s) (1 20%) 0.3322\n",
      "59m 51s (- 239m 25s) (1 20%) 0.3412\n",
      "60m 47s (- 243m 8s) (1 20%) 0.3165\n",
      "61m 43s (- 246m 52s) (1 20%) 0.3387\n",
      "62m 38s (- 250m 32s) (1 20%) 0.3301\n",
      "63m 33s (- 254m 13s) (1 20%) 0.3174\n",
      "64m 28s (- 257m 53s) (1 20%) 0.3422\n",
      "65m 23s (- 261m 32s) (1 20%) 0.3234\n",
      "66m 17s (- 265m 11s) (1 20%) 0.3388\n",
      "67m 12s (- 268m 50s) (1 20%) 0.3247\n",
      "68m 7s (- 272m 29s) (1 20%) 0.3280\n",
      "69m 2s (- 276m 8s) (1 20%) 0.3106\n",
      "69m 57s (- 279m 48s) (1 20%) 0.3269\n",
      "70m 51s (- 283m 27s) (1 20%) 0.3193\n",
      "71m 47s (- 287m 8s) (1 20%) 0.3326\n",
      "72m 42s (- 290m 51s) (1 20%) 0.3338\n",
      "73m 38s (- 294m 34s) (1 20%) 0.3279\n",
      "74m 34s (- 298m 16s) (1 20%) 0.3244\n",
      "75m 29s (- 301m 58s) (1 20%) 0.3268\n",
      "76m 25s (- 305m 40s) (1 20%) 0.3163\n",
      "77m 20s (- 309m 22s) (1 20%) 0.3208\n",
      "78m 16s (- 313m 5s) (1 20%) 0.3259\n",
      "79m 11s (- 316m 47s) (1 20%) 0.3125\n",
      "80m 7s (- 320m 29s) (1 20%) 0.3245\n",
      "81m 3s (- 324m 12s) (1 20%) 0.3286\n",
      "81m 58s (- 327m 54s) (1 20%) 0.3215\n",
      "82m 54s (- 331m 39s) (1 20%) 0.3337\n",
      "83m 51s (- 335m 24s) (1 20%) 0.3207\n",
      "84m 45s (- 339m 3s) (1 20%) 0.3179\n",
      "85m 40s (- 342m 42s) (1 20%) 0.3155\n",
      "86m 35s (- 346m 21s) (1 20%) 0.3182\n",
      "87m 30s (- 350m 2s) (1 20%) 0.3244\n",
      "88m 25s (- 353m 41s) (1 20%) 0.3146\n",
      "89m 20s (- 357m 21s) (1 20%) 0.3119\n",
      "90m 15s (- 361m 0s) (1 20%) 0.3117\n",
      "91m 10s (- 364m 41s) (1 20%) 0.3071\n",
      "92m 5s (- 368m 20s) (1 20%) 0.3081\n",
      "92m 59s (- 371m 59s) (1 20%) 0.3074\n",
      "93m 55s (- 375m 40s) (1 20%) 0.3234\n",
      "94m 50s (- 379m 22s) (1 20%) 0.3176\n",
      "95m 46s (- 383m 4s) (1 20%) 0.3085\n",
      "96m 41s (- 386m 46s) (1 20%) 0.3073\n",
      "97m 37s (- 390m 28s) (1 20%) 0.3011\n",
      "98m 32s (- 394m 9s) (1 20%) 0.3003\n",
      "99m 27s (- 397m 50s) (1 20%) 0.3092\n",
      "100m 23s (- 401m 32s) (1 20%) 0.3091\n",
      "101m 18s (- 405m 14s) (1 20%) 0.3161\n",
      "102m 13s (- 408m 55s) (1 20%) 0.3081\n",
      "103m 9s (- 412m 37s) (1 20%) 0.3098\n",
      "104m 5s (- 416m 20s) (1 20%) 0.3097\n",
      "105m 1s (- 420m 5s) (1 20%) 0.3047\n",
      "105m 57s (- 423m 51s) (1 20%) 0.3244\n",
      "106m 52s (- 427m 31s) (1 20%) 0.3110\n",
      "107m 47s (- 431m 10s) (1 20%) 0.3051\n",
      "108m 42s (- 434m 51s) (1 20%) 0.3112\n",
      "109m 37s (- 438m 31s) (1 20%) 0.3151\n",
      "110m 33s (- 442m 12s) (1 20%) 0.3325\n",
      "111m 27s (- 445m 51s) (1 20%) 0.2993\n",
      "112m 22s (- 449m 28s) (1 20%) 0.2948\n",
      "113m 17s (- 453m 8s) (1 20%) 0.3037\n",
      "114m 11s (- 456m 47s) (1 20%) 0.3104\n",
      "115m 7s (- 460m 28s) (1 20%) 0.3195\n",
      "116m 2s (- 464m 9s) (1 20%) 0.3171\n",
      "116m 57s (- 467m 51s) (1 20%) 0.3008\n",
      "117m 53s (- 471m 34s) (1 20%) 0.3228\n",
      "118m 49s (- 475m 16s) (1 20%) 0.3133\n",
      "119m 44s (- 478m 57s) (1 20%) 0.3061\n",
      "120m 39s (- 482m 39s) (1 20%) 0.3032\n",
      "121m 35s (- 486m 20s) (1 20%) 0.3004\n",
      "122m 30s (- 490m 2s) (1 20%) 0.3016\n",
      "122m 50s (- 184m 15s) (2 40%) 0.1044\n",
      "123m 45s (- 185m 38s) (2 40%) 0.3108\n",
      "124m 41s (- 187m 2s) (2 40%) 0.2922\n",
      "125m 37s (- 188m 25s) (2 40%) 0.3136\n",
      "126m 32s (- 189m 49s) (2 40%) 0.3116\n",
      "127m 28s (- 191m 13s) (2 40%) 0.3050\n",
      "128m 24s (- 192m 37s) (2 40%) 0.2865\n",
      "129m 20s (- 194m 0s) (2 40%) 0.2985\n",
      "130m 14s (- 195m 22s) (2 40%) 0.2963\n",
      "131m 9s (- 196m 44s) (2 40%) 0.3057\n",
      "132m 4s (- 198m 7s) (2 40%) 0.3015\n",
      "132m 59s (- 199m 29s) (2 40%) 0.3039\n",
      "133m 54s (- 200m 52s) (2 40%) 0.3013\n",
      "134m 49s (- 202m 14s) (2 40%) 0.2964\n",
      "135m 44s (- 203m 37s) (2 40%) 0.3014\n",
      "136m 39s (- 204m 59s) (2 40%) 0.3043\n",
      "137m 34s (- 206m 21s) (2 40%) 0.2946\n",
      "138m 29s (- 207m 44s) (2 40%) 0.2805\n",
      "139m 24s (- 209m 7s) (2 40%) 0.2975\n",
      "140m 20s (- 210m 30s) (2 40%) 0.3143\n",
      "141m 16s (- 211m 54s) (2 40%) 0.3120\n",
      "142m 11s (- 213m 17s) (2 40%) 0.2872\n",
      "143m 7s (- 214m 40s) (2 40%) 0.3052\n",
      "144m 2s (- 216m 3s) (2 40%) 0.3015\n",
      "144m 58s (- 217m 27s) (2 40%) 0.2959\n",
      "145m 53s (- 218m 50s) (2 40%) 0.3015\n",
      "146m 49s (- 220m 13s) (2 40%) 0.2991\n",
      "147m 44s (- 221m 36s) (2 40%) 0.2954\n",
      "148m 40s (- 223m 0s) (2 40%) 0.3100\n",
      "149m 35s (- 224m 23s) (2 40%) 0.2942\n",
      "150m 32s (- 225m 48s) (2 40%) 0.2970\n",
      "151m 27s (- 227m 11s) (2 40%) 0.3090\n",
      "152m 22s (- 228m 34s) (2 40%) 0.2945\n",
      "153m 17s (- 229m 56s) (2 40%) 0.2973\n",
      "154m 12s (- 231m 19s) (2 40%) 0.2807\n",
      "155m 7s (- 232m 41s) (2 40%) 0.2798\n",
      "156m 2s (- 234m 3s) (2 40%) 0.2996\n",
      "156m 57s (- 235m 26s) (2 40%) 0.2907\n",
      "157m 52s (- 236m 48s) (2 40%) 0.2875\n",
      "158m 47s (- 238m 10s) (2 40%) 0.2818\n",
      "159m 42s (- 239m 33s) (2 40%) 0.2974\n",
      "160m 37s (- 240m 55s) (2 40%) 0.2778\n",
      "161m 32s (- 242m 19s) (2 40%) 0.3139\n",
      "162m 28s (- 243m 42s) (2 40%) 0.2973\n",
      "163m 23s (- 245m 5s) (2 40%) 0.2854\n",
      "164m 19s (- 246m 29s) (2 40%) 0.2851\n",
      "165m 15s (- 247m 52s) (2 40%) 0.2927\n",
      "166m 10s (- 249m 16s) (2 40%) 0.2928\n",
      "167m 6s (- 250m 39s) (2 40%) 0.2825\n",
      "168m 1s (- 252m 2s) (2 40%) 0.2827\n",
      "168m 57s (- 253m 26s) (2 40%) 0.2970\n",
      "169m 53s (- 254m 49s) (2 40%) 0.2880\n",
      "170m 48s (- 256m 13s) (2 40%) 0.2893\n",
      "171m 44s (- 257m 36s) (2 40%) 0.2889\n",
      "172m 40s (- 259m 0s) (2 40%) 0.2930\n",
      "173m 36s (- 260m 24s) (2 40%) 0.2896\n",
      "174m 31s (- 261m 46s) (2 40%) 0.2831\n",
      "175m 26s (- 263m 9s) (2 40%) 0.2919\n",
      "176m 21s (- 264m 31s) (2 40%) 0.3042\n",
      "177m 15s (- 265m 53s) (2 40%) 0.2778\n",
      "178m 10s (- 267m 16s) (2 40%) 0.2852\n",
      "179m 5s (- 268m 38s) (2 40%) 0.2903\n",
      "180m 0s (- 270m 0s) (2 40%) 0.2871\n",
      "180m 55s (- 271m 23s) (2 40%) 0.2950\n",
      "181m 50s (- 272m 46s) (2 40%) 0.2902\n",
      "182m 45s (- 274m 8s) (2 40%) 0.2723\n",
      "183m 40s (- 275m 31s) (2 40%) 0.2727\n",
      "184m 36s (- 276m 54s) (2 40%) 0.2828\n",
      "185m 32s (- 278m 18s) (2 40%) 0.2941\n",
      "186m 27s (- 279m 41s) (2 40%) 0.2819\n",
      "187m 23s (- 281m 4s) (2 40%) 0.2863\n",
      "188m 18s (- 282m 28s) (2 40%) 0.2931\n",
      "189m 14s (- 283m 51s) (2 40%) 0.2816\n",
      "190m 9s (- 285m 14s) (2 40%) 0.2615\n",
      "191m 5s (- 286m 37s) (2 40%) 0.2766\n",
      "192m 0s (- 288m 0s) (2 40%) 0.2960\n",
      "192m 56s (- 289m 24s) (2 40%) 0.2870\n",
      "193m 51s (- 290m 47s) (2 40%) 0.2806\n",
      "194m 47s (- 292m 11s) (2 40%) 0.2960\n",
      "195m 43s (- 293m 35s) (2 40%) 0.2914\n",
      "196m 38s (- 294m 58s) (2 40%) 0.2925\n",
      "197m 33s (- 296m 20s) (2 40%) 0.3044\n",
      "198m 28s (- 297m 42s) (2 40%) 0.2720\n",
      "199m 23s (- 299m 5s) (2 40%) 0.2728\n",
      "200m 18s (- 300m 27s) (2 40%) 0.2814\n",
      "201m 13s (- 301m 49s) (2 40%) 0.2880\n",
      "202m 7s (- 303m 11s) (2 40%) 0.2753\n",
      "203m 2s (- 304m 33s) (2 40%) 0.2851\n",
      "203m 57s (- 305m 56s) (2 40%) 0.2776\n",
      "204m 52s (- 307m 18s) (2 40%) 0.2865\n",
      "205m 47s (- 308m 41s) (2 40%) 0.2706\n",
      "206m 43s (- 310m 4s) (2 40%) 0.2803\n",
      "207m 38s (- 311m 27s) (2 40%) 0.2800\n",
      "208m 33s (- 312m 50s) (2 40%) 0.2849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209m 29s (- 314m 14s) (2 40%) 0.2847\n",
      "210m 24s (- 315m 36s) (2 40%) 0.2647\n",
      "211m 20s (- 317m 0s) (2 40%) 0.2986\n",
      "212m 15s (- 318m 23s) (2 40%) 0.2834\n",
      "213m 11s (- 319m 46s) (2 40%) 0.2834\n",
      "214m 6s (- 321m 9s) (2 40%) 0.2840\n",
      "215m 2s (- 322m 33s) (2 40%) 0.2824\n",
      "215m 57s (- 323m 56s) (2 40%) 0.2776\n",
      "216m 53s (- 325m 20s) (2 40%) 0.2806\n",
      "217m 49s (- 326m 44s) (2 40%) 0.2818\n",
      "218m 45s (- 328m 7s) (2 40%) 0.2867\n",
      "219m 40s (- 329m 30s) (2 40%) 0.2933\n",
      "220m 35s (- 330m 53s) (2 40%) 0.2749\n",
      "221m 30s (- 332m 15s) (2 40%) 0.2825\n",
      "222m 25s (- 333m 37s) (2 40%) 0.2701\n",
      "223m 20s (- 335m 0s) (2 40%) 0.2715\n",
      "224m 14s (- 336m 22s) (2 40%) 0.2810\n",
      "225m 9s (- 337m 44s) (2 40%) 0.2799\n",
      "226m 4s (- 339m 7s) (2 40%) 0.2790\n",
      "226m 59s (- 340m 29s) (2 40%) 0.2893\n",
      "227m 54s (- 341m 52s) (2 40%) 0.2818\n",
      "228m 50s (- 343m 15s) (2 40%) 0.2685\n",
      "229m 45s (- 344m 38s) (2 40%) 0.2796\n",
      "230m 41s (- 346m 1s) (2 40%) 0.2754\n",
      "231m 36s (- 347m 24s) (2 40%) 0.2614\n",
      "232m 32s (- 348m 48s) (2 40%) 0.2856\n",
      "233m 27s (- 350m 11s) (2 40%) 0.2806\n",
      "234m 23s (- 351m 35s) (2 40%) 0.2706\n",
      "235m 18s (- 352m 58s) (2 40%) 0.2783\n",
      "236m 14s (- 354m 21s) (2 40%) 0.2827\n",
      "237m 9s (- 355m 44s) (2 40%) 0.2718\n",
      "238m 5s (- 357m 8s) (2 40%) 0.2856\n",
      "239m 1s (- 358m 31s) (2 40%) 0.2738\n",
      "239m 56s (- 359m 55s) (2 40%) 0.2767\n",
      "240m 53s (- 361m 19s) (2 40%) 0.2838\n",
      "241m 48s (- 362m 43s) (2 40%) 0.2853\n",
      "242m 43s (- 364m 5s) (2 40%) 0.2846\n",
      "243m 38s (- 365m 28s) (2 40%) 0.2798\n",
      "244m 34s (- 366m 51s) (2 40%) 0.2871\n",
      "245m 29s (- 368m 13s) (2 40%) 0.2802\n",
      "245m 49s (- 163m 52s) (3 60%) 0.1024\n",
      "246m 43s (- 164m 29s) (3 60%) 0.2567\n",
      "247m 38s (- 165m 5s) (3 60%) 0.2639\n",
      "248m 33s (- 165m 42s) (3 60%) 0.2560\n",
      "249m 28s (- 166m 18s) (3 60%) 0.2716\n",
      "250m 23s (- 166m 55s) (3 60%) 0.2624\n",
      "251m 18s (- 167m 32s) (3 60%) 0.2810\n",
      "252m 13s (- 168m 9s) (3 60%) 0.2662\n",
      "253m 9s (- 168m 46s) (3 60%) 0.2718\n",
      "254m 4s (- 169m 22s) (3 60%) 0.2645\n",
      "255m 0s (- 170m 0s) (3 60%) 0.2798\n",
      "255m 55s (- 170m 37s) (3 60%) 0.2681\n",
      "256m 51s (- 171m 14s) (3 60%) 0.2523\n",
      "257m 46s (- 171m 51s) (3 60%) 0.2670\n",
      "258m 41s (- 172m 27s) (3 60%) 0.2588\n",
      "259m 37s (- 173m 4s) (3 60%) 0.2579\n",
      "260m 32s (- 173m 41s) (3 60%) 0.2813\n",
      "261m 28s (- 174m 18s) (3 60%) 0.2778\n",
      "262m 23s (- 174m 55s) (3 60%) 0.2736\n",
      "263m 19s (- 175m 33s) (3 60%) 0.2834\n",
      "264m 15s (- 176m 10s) (3 60%) 0.2601\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "encoder1 = EncoderRNN(EMBEDDING_SIZE,hidden_size).to(device)\n",
    "decoder1 = AttnDecoderRNN(EMBEDDING_SIZE,hidden_size, len(ordered_words_ft)).to(device)\n",
    "\n",
    "##UNCOMMENT TO TRAIN THE MODEL\n",
    "trainIters(encoder1, decoder1, 5, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
