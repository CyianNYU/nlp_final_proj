{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re  \n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install sacrebleu\n",
    "from sacrebleu import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_load = 50000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "\n",
    "with open('cc.zh.300.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+3, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    ordered_words_ft.extend(['<pad>', '<unk>', '<s>'])\n",
    "    loaded_embeddings_ft[0,:] = np.zeros(300)\n",
    "    loaded_embeddings_ft[1,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft[2,:] = np.random.normal(size = 300)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+3, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+3\n",
    "        idx2words_ft[i+3] = s[0]\n",
    "        ordered_words_ft.append(s[0])\n",
    "    words_ft['<pad>'] = PAD_IDX\n",
    "    words_ft['<unk>'] = UNK_IDX\n",
    "    words_ft['<s>'] = SOS_IDX\n",
    "    idx2words_ft[PAD_IDX] = '<pad>'\n",
    "    idx2words_ft[UNK_IDX] = '<unk>'\n",
    "    idx2words_ft[SOS_IDX] = '<s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English embedding\n",
    "with open('wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft_en = np.zeros((words_to_load+4, 300))\n",
    "    words_ft_en = {}\n",
    "    idx2words_ft_en = {}\n",
    "    ordered_words_ft_en = []\n",
    "    ordered_words_ft_en.extend(['<pad>', '<unk>', '<s>', '</s>'])\n",
    "    loaded_embeddings_ft_en[0,:] = np.zeros(300)\n",
    "    loaded_embeddings_ft_en[1,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en[2,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en[3,:] = np.random.normal(size = 300)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft_en[i+4, :] = np.asarray(s[1:])\n",
    "        words_ft_en[s[0]] = i+4\n",
    "        idx2words_ft_en[i+4] = s[0]\n",
    "        ordered_words_ft_en.append(s[0])\n",
    "    words_ft_en['<pad>'] = PAD_IDX\n",
    "    words_ft_en['<unk>'] = UNK_IDX\n",
    "    words_ft_en['<s>'] = SOS_IDX\n",
    "    words_ft_en['</s>'] = EOS_IDX\n",
    "    idx2words_ft_en[PAD_IDX] = '<pad>'\n",
    "    idx2words_ft_en[UNK_IDX] = '<unk>'\n",
    "    idx2words_ft_en[SOS_IDX] = '<s>'\n",
    "    idx2words_ft_en[EOS_IDX] = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in chinese-english pairs\n",
    "#read in chinese-english pairs\n",
    "lines_zh = open('iwslt-zh-en/train.tok.zh',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en = open('iwslt-zh-en/train.tok.en',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_zh_test = open('iwslt-zh-en/test.tok.zh',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_test = open('iwslt-zh-en/test.tok.en',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_zh_val = open('iwslt-zh-en/dev.tok.zh',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_val = open('iwslt-zh-en/dev.tok.en',encoding = 'utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add sos and eos in each sentence\n",
    "def add_sos_eos(lines):\n",
    "    \n",
    "    train = []\n",
    "    for l in lines:\n",
    "        l = '<s> ' + l + '</s>'\n",
    "        train.append(l)\n",
    "    return train\n",
    "zh_train = add_sos_eos(lines_zh)    \n",
    "en_train = add_sos_eos(lines_en)\n",
    "zh_test = add_sos_eos(lines_zh_test)\n",
    "en_test = add_sos_eos(lines_en_test)\n",
    "zh_val = add_sos_eos(lines_zh_val)\n",
    "en_val = add_sos_eos(lines_en_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data,eng = False):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = []\n",
    "        for token in tokens.split():\n",
    "            if eng == False:\n",
    "                try:\n",
    "                    index_list.append(words_ft[token])\n",
    "                except KeyError:\n",
    "                    index_list.append(UNK_IDX)\n",
    "            else:\n",
    "                try:\n",
    "                    index_list.append(words_ft_en[token])\n",
    "                except KeyError:\n",
    "                    index_list.append(UNK_IDX)\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_train_indices = token2index_dataset(zh_train)\n",
    "en_train_indices = token2index_dataset(en_train,eng = True)\n",
    "zh_test_indices = token2index_dataset(zh_test)\n",
    "en_test_indices = token2index_dataset(en_test,eng = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_sentence_length\n",
    "length_of_en = [len(x.split()) for x in en_train]\n",
    "max_sentence_length_en = sorted(length_of_en)[-int(len(length_of_en)*0.01)]\n",
    "length_of_zh = [len(x.split()) for x in zh_train]\n",
    "max_sentence_length_zh = sorted(length_of_zh)[-int(len(length_of_zh)*0.01)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Data Loader\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class load_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list_s1,data_list_s2):\n",
    "        \"\"\"\n",
    "        @param data_list_zh: list of Chinese tokens \n",
    "        @param data_list_en: list of English tokens as TARGETS\n",
    "        \"\"\"\n",
    "        self.data_list_s1 = data_list_s1\n",
    "        self.data_list_s2 = data_list_s2\n",
    "        \n",
    "        assert (len(self.data_list_s1) == len(self.data_list_s2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list_s1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx_s1 = self.data_list_s1[key][:max_sentence_length_zh]\n",
    "        token_idx_s2 = self.data_list_s2[key][:max_sentence_length_en]\n",
    "        return [token_idx_s1, token_idx_s2, len(token_idx_s1), len(token_idx_s2)]\n",
    "\n",
    "def collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list_s1 = []\n",
    "    data_list_s2 = []\n",
    "    length_list_s1 = []\n",
    "    length_list_s2 = []\n",
    "    for datum in batch:\n",
    "        length_list_s1.append(datum[2])\n",
    "        length_list_s2.append(datum[3])\n",
    "        padded_vec_zh = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,max_sentence_length_zh-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_en = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,max_sentence_length_en-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_s1.append(padded_vec_zh[:max_sentence_length_zh])\n",
    "        data_list_s2.append(padded_vec_en[:max_sentence_length_en])\n",
    "    #print(type(data_list_s1[0]))\n",
    "    if torch.cuda.is_available and torch.has_cudnn:\n",
    "        return [torch.from_numpy(np.array(data_list_s1)).cuda(), torch.from_numpy(np.array(data_list_s2)).cuda(),\n",
    "                torch.LongTensor(length_list_s1).cuda(), torch.LongTensor(length_list_s2).cuda()]\n",
    "    else:    \n",
    "        return [torch.from_numpy(np.array(data_list_s1)), torch.from_numpy(np.array(data_list_s2)),\n",
    "                torch.LongTensor(length_list_s1), torch.LongTensor(length_list_s2)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "EMBEDDING_SIZE = 300 # fixed as from the input embedding data\n",
    "\n",
    "train_dataset = load_dataset(zh_train_indices, en_train_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = load_dataset(zh_test_indices, en_test_indices)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_size, embed= torch.from_numpy(loaded_embeddings_ft).float(),num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers \n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed, freeze=True)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_size,num_layers=num_layers,batch_first=True)\n",
    "\n",
    "    def forward(self, data, hidden):\n",
    "        \n",
    "        batch_size, seq_len = data.size()\n",
    "        \n",
    "        embed = self.embedding(data)\n",
    "        \n",
    "        output, hidden = self.gru(embed,hidden)\n",
    "        #hidden = [n layers * n directions =1 , batch_size, hidden_size ]\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    # initialize the hidden with random numbers\n",
    "    def initHidden(self,batch_size):\n",
    "        return torch.randn(self.num_layers, batch_size, self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,emb_dim,hidden_size, output_size, embed= torch.from_numpy(loaded_embeddings_ft_en).float(),num_layers=1,\n",
    "                 dropout_p=0.1, max_length=max_sentence_length_zh):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers \n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed, freeze=False)\n",
    "        self.attn = nn.Linear(self.hidden_size + emb_dim, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size + emb_dim, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, data, hidden,encoder_outputs):\n",
    "        \n",
    "        ### embed: [1 * batch size * emb_dim = 300 ] ###\n",
    "        ### hidden: [1 * batch size * hidden_size = 300 ] ###\n",
    "        ### encoder_outputs: [batch size * max_sentence_length_zh * hidden_size = 300 ] ###\n",
    "        ### 因为这里concat之后，attn layer 他给的是 hidden size *2 \n",
    "        ### 所以我这儿的hidden size就只能写300了 \n",
    "        \n",
    "        embed = self.embedding(data)\n",
    "        embed = self.dropout(embed)    \n",
    "        ### torch.cat((embed, hidden), 2)  \n",
    "        ### [1 * batch size * (emb_dim + hidden_size) ]\n",
    "        \n",
    "        ### attn_weights: [1 * batch size * max_sentence_length_zh ]###\n",
    "        ### attn_weights[0].unsqueeze(1): [batch size * 1 * max_sentence_length_zh ]###\n",
    "        \n",
    "        ### softmax dim=2 因为最后一个dimension是 词组什么的，不能是1，1的话就是\n",
    "        ### 不同batch间这样比较了？\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embed, hidden), 2)), dim=2)\n",
    "        \n",
    "\n",
    "        ### torch.bmm(attn_weights[0].unsqueeze(1),encoder_outputs).squeeze(1) :\n",
    "        ### [batch size * 1 * hidden_size ]###\n",
    "\n",
    "        ### attn_applied: [batch size * hidden_size (= 300) ] ###\n",
    "     \n",
    "        attn_applied = torch.bmm(attn_weights[0].unsqueeze(1),\n",
    "                                 encoder_outputs).squeeze(1)\n",
    "        \n",
    "        ### output: [batch size * hidden_size (= 300) ] ###\n",
    "        ### embed[0]: [batch size * hidden_size (= 300) ] ###\n",
    "\n",
    "        output = torch.cat((embed[0], attn_applied), 1)\n",
    " \n",
    "        ### output: [1 * batch size * hidden_size (= 300) ] ###\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        ### output: [1 * batch size * hidden_size (= 300) ] ###\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        #print(hidden.size())\n",
    "        #print(output.size())\n",
    "\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return torch.randn(self.num_layers, batch_size, self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden_size = 220\n",
    "#encoder1 = EncoderRNN(EMBEDDING_SIZE,hidden_size).to(device)\n",
    "#decoder1 = AttnDecoderRNN(EMBEDDING_SIZE,hidden_size, len(ordered_words_ft)).to(device)\n",
    "\n",
    "##UNCOMMENT TO TRAIN THE MODEL\n",
    "#trainIters(encoder1, decoder1, 5, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "#input_tensor: list of sentence tensor\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "          criterion,eee):\n",
    "    \n",
    "    ### target_tensor [batch size, max_sentence_length_en = 73] ###\n",
    "    ### target_tensor [batch size, max_sentence_length_zh = 62] ###\n",
    "    batch_size_1, input_length = input_tensor.size()\n",
    "    batch_size_2, target_length = target_tensor.size()\n",
    "    \n",
    "    \n",
    "    encoder_hidden = encoder.initHidden(batch_size_1)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    ### encoder_hidden: 1 * batch * hidden size ### \n",
    "    ### encoder_output: batch size * max_sentence_length_zh * hidden size ### \n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor(np.array([[SOS_IDX]]*batch_size_1).reshape(1,batch_size_1),device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    #print(use_teacher_forcing)\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            \n",
    "            ### decoder_output: [batchsize,5000] ###\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden,encoder_output)\n",
    "        \n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            decoder_input = target_tensor[:,di].unsqueeze(0)  # Teacher forcing\n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden,encoder_output)\n",
    "                        \n",
    "            ### decoder_output [batch size, 50003]  ###\n",
    "            \n",
    "            ### topi is a [batch size, 1] tensor first we remove the size 1\n",
    "            ### demension then we add it at the beginning using squeeze\n",
    "            ### 有点脑残诶，做个转置不就好了？\n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            \n",
    "            ### decoder_input [1, batch size]  ###\n",
    "            decoder_input = decoder_input.unsqueeze(0)\n",
    " \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "    #--------------------------------------------\t\n",
    "    #\t\n",
    "    #    LOAD MODELS\t\n",
    "    #\t\n",
    "    #--------------------------------------------\t\n",
    "    folder = './attentation_model'\t\n",
    "    if not os.path.exists(folder):\t\n",
    "        os.makedirs(folder)\t\n",
    "\n",
    "    if os.path.exists('./attentation_model/Encoder_b'):\t\n",
    "        print('---------------------------------------------------------------------')\t\n",
    "        print('----------------Readind trained model---------------------------------')\t\n",
    "        print('---------------------------------------------------------------------')\t\n",
    "        \t\n",
    "        #read trained models\t\n",
    "        encoder.load_state_dict(torch.load(folder+\"/Encoder_b\"))\n",
    "        decoder.load_state_dict(torch.load(folder+\"/Decoder_b\"))\t\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for i, (data_s1, data_s2, lengths_s1, lengths_s2) in enumerate(train_loader):\n",
    "            input_tensor = data_s1\n",
    "            target_tensor = data_s2\n",
    "            #print(\"train\",target_tensor.size())\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion,i)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                             iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            if i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                \n",
    "        # Save the model for every epoch\n",
    "        print('---------------------------------------------------------------------')\t\n",
    "        print('----------------Saving trained model---------------------------------')\t\n",
    "        print('---------------------------------------------------------------------')\t\n",
    "      \n",
    "        torch.save(encoder.state_dict(),folder +\"/Encoder_b\")\n",
    "        torch.save(decoder.state_dict(),folder +\"/Decoder_b\")\n",
    "\n",
    "    \n",
    "    return plot_losses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "----------------Readind trained model---------------------------------\n",
      "---------------------------------------------------------------------\n",
      "0m 0s (- 0m 2s) (1 20%) 0.0290\n",
      "0m 15s (- 1m 3s) (1 20%) 1.2019\n",
      "0m 31s (- 2m 4s) (1 20%) 1.1659\n",
      "0m 46s (- 3m 4s) (1 20%) 1.1139\n",
      "1m 1s (- 4m 4s) (1 20%) 1.1222\n",
      "1m 16s (- 5m 5s) (1 20%) 1.1455\n",
      "1m 31s (- 6m 6s) (1 20%) 1.1891\n",
      "1m 46s (- 7m 7s) (1 20%) 1.1754\n",
      "2m 2s (- 8m 8s) (1 20%) 1.1633\n",
      "2m 17s (- 9m 9s) (1 20%) 1.1709\n",
      "2m 32s (- 10m 10s) (1 20%) 1.2182\n",
      "2m 47s (- 11m 11s) (1 20%) 1.2101\n",
      "3m 3s (- 12m 12s) (1 20%) 1.1054\n",
      "3m 18s (- 13m 12s) (1 20%) 1.1505\n",
      "3m 33s (- 14m 14s) (1 20%) 1.2002\n",
      "3m 48s (- 15m 15s) (1 20%) 1.1955\n",
      "4m 3s (- 16m 15s) (1 20%) 1.1632\n",
      "4m 19s (- 17m 16s) (1 20%) 1.1443\n",
      "4m 34s (- 18m 17s) (1 20%) 1.1283\n",
      "4m 49s (- 19m 17s) (1 20%) 1.1140\n",
      "5m 4s (- 20m 18s) (1 20%) 1.1800\n",
      "5m 19s (- 21m 19s) (1 20%) 1.2271\n",
      "5m 35s (- 22m 20s) (1 20%) 1.1895\n",
      "5m 50s (- 23m 20s) (1 20%) 1.1040\n",
      "6m 5s (- 24m 20s) (1 20%) 1.1242\n",
      "6m 20s (- 25m 21s) (1 20%) 1.1838\n",
      "6m 35s (- 26m 22s) (1 20%) 1.1738\n",
      "6m 50s (- 27m 23s) (1 20%) 1.1819\n",
      "7m 6s (- 28m 24s) (1 20%) 1.1168\n",
      "7m 21s (- 29m 25s) (1 20%) 1.2019\n",
      "7m 36s (- 30m 25s) (1 20%) 1.1422\n",
      "7m 51s (- 31m 26s) (1 20%) 1.1925\n",
      "8m 6s (- 32m 27s) (1 20%) 1.1461\n",
      "8m 21s (- 33m 27s) (1 20%) 1.1730\n",
      "8m 37s (- 34m 28s) (1 20%) 1.1595\n",
      "8m 52s (- 35m 29s) (1 20%) 1.2153\n",
      "9m 7s (- 36m 31s) (1 20%) 1.2602\n",
      "9m 22s (- 37m 31s) (1 20%) 1.1203\n",
      "9m 38s (- 38m 32s) (1 20%) 1.2270\n",
      "9m 53s (- 39m 33s) (1 20%) 1.1979\n",
      "10m 8s (- 40m 34s) (1 20%) 1.1827\n",
      "10m 23s (- 41m 34s) (1 20%) 1.1252\n",
      "10m 38s (- 42m 35s) (1 20%) 1.2228\n",
      "---------------------------------------------------------------------\n",
      "----------------Saving trained model---------------------------------\n",
      "---------------------------------------------------------------------\n",
      "10m 49s (- 16m 14s) (2 40%) 0.8143\n",
      "11m 4s (- 16m 37s) (2 40%) 1.1034\n",
      "11m 20s (- 17m 0s) (2 40%) 1.0602\n",
      "11m 35s (- 17m 22s) (2 40%) 1.0991\n",
      "11m 50s (- 17m 45s) (2 40%) 1.1723\n",
      "12m 5s (- 18m 8s) (2 40%) 1.0862\n",
      "12m 20s (- 18m 30s) (2 40%) 1.1196\n",
      "12m 35s (- 18m 53s) (2 40%) 1.1863\n",
      "12m 51s (- 19m 16s) (2 40%) 1.1802\n",
      "13m 6s (- 19m 39s) (2 40%) 1.1339\n",
      "13m 21s (- 20m 2s) (2 40%) 1.1470\n",
      "13m 36s (- 20m 25s) (2 40%) 1.1771\n",
      "13m 52s (- 20m 48s) (2 40%) 1.1757\n",
      "14m 7s (- 21m 11s) (2 40%) 1.1485\n",
      "14m 22s (- 21m 33s) (2 40%) 1.1567\n",
      "14m 37s (- 21m 56s) (2 40%) 1.0556\n",
      "14m 52s (- 22m 19s) (2 40%) 1.1288\n",
      "15m 8s (- 22m 42s) (2 40%) 1.1727\n",
      "15m 23s (- 23m 4s) (2 40%) 1.0871\n",
      "15m 38s (- 23m 27s) (2 40%) 1.1568\n",
      "15m 53s (- 23m 50s) (2 40%) 1.1161\n",
      "16m 8s (- 24m 12s) (2 40%) 1.0692\n",
      "16m 23s (- 24m 35s) (2 40%) 1.1378\n",
      "16m 38s (- 24m 58s) (2 40%) 1.1115\n",
      "16m 54s (- 25m 21s) (2 40%) 1.1764\n",
      "17m 9s (- 25m 44s) (2 40%) 1.1847\n",
      "17m 24s (- 26m 6s) (2 40%) 1.1136\n",
      "17m 39s (- 26m 29s) (2 40%) 1.0974\n",
      "17m 54s (- 26m 52s) (2 40%) 1.0862\n",
      "18m 9s (- 27m 14s) (2 40%) 1.1691\n",
      "18m 25s (- 27m 37s) (2 40%) 1.2028\n",
      "18m 40s (- 28m 0s) (2 40%) 1.1790\n",
      "18m 55s (- 28m 23s) (2 40%) 1.1403\n",
      "19m 10s (- 28m 46s) (2 40%) 1.1445\n",
      "19m 26s (- 29m 9s) (2 40%) 1.2132\n",
      "19m 41s (- 29m 32s) (2 40%) 1.2006\n",
      "19m 56s (- 29m 54s) (2 40%) 1.1124\n",
      "20m 11s (- 30m 17s) (2 40%) 1.1580\n",
      "20m 26s (- 30m 40s) (2 40%) 1.1174\n",
      "20m 42s (- 31m 3s) (2 40%) 1.1836\n",
      "20m 57s (- 31m 26s) (2 40%) 1.1631\n",
      "21m 12s (- 31m 48s) (2 40%) 1.1755\n",
      "21m 27s (- 32m 11s) (2 40%) 1.1284\n",
      "---------------------------------------------------------------------\n",
      "----------------Saving trained model---------------------------------\n",
      "---------------------------------------------------------------------\n",
      "21m 38s (- 14m 25s) (3 60%) 0.7845\n",
      "21m 53s (- 14m 35s) (3 60%) 1.1056\n",
      "22m 9s (- 14m 46s) (3 60%) 1.1629\n",
      "22m 24s (- 14m 56s) (3 60%) 1.0742\n",
      "22m 39s (- 15m 6s) (3 60%) 1.1216\n",
      "22m 54s (- 15m 16s) (3 60%) 1.0736\n",
      "23m 9s (- 15m 26s) (3 60%) 1.1327\n",
      "23m 24s (- 15m 36s) (3 60%) 1.0949\n",
      "23m 40s (- 15m 46s) (3 60%) 1.1326\n",
      "23m 55s (- 15m 56s) (3 60%) 1.0913\n",
      "24m 10s (- 16m 6s) (3 60%) 1.0921\n",
      "24m 25s (- 16m 17s) (3 60%) 1.1762\n",
      "24m 41s (- 16m 27s) (3 60%) 1.1231\n",
      "24m 56s (- 16m 37s) (3 60%) 1.0972\n",
      "25m 11s (- 16m 47s) (3 60%) 1.1056\n",
      "25m 26s (- 16m 57s) (3 60%) 1.1215\n",
      "25m 41s (- 17m 7s) (3 60%) 1.1338\n",
      "25m 56s (- 17m 17s) (3 60%) 1.1972\n",
      "26m 12s (- 17m 28s) (3 60%) 1.0999\n",
      "26m 27s (- 17m 38s) (3 60%) 1.0658\n",
      "26m 42s (- 17m 48s) (3 60%) 1.1570\n",
      "26m 57s (- 17m 58s) (3 60%) 1.0850\n",
      "27m 12s (- 18m 8s) (3 60%) 1.1560\n",
      "27m 27s (- 18m 18s) (3 60%) 1.1033\n",
      "27m 43s (- 18m 28s) (3 60%) 1.1730\n",
      "27m 58s (- 18m 38s) (3 60%) 1.1706\n",
      "28m 13s (- 18m 49s) (3 60%) 1.1008\n",
      "28m 28s (- 18m 59s) (3 60%) 1.1378\n",
      "28m 44s (- 19m 9s) (3 60%) 1.1398\n",
      "28m 59s (- 19m 19s) (3 60%) 1.1582\n",
      "29m 14s (- 19m 29s) (3 60%) 1.1095\n",
      "29m 29s (- 19m 39s) (3 60%) 1.1223\n",
      "29m 44s (- 19m 49s) (3 60%) 1.0955\n",
      "29m 59s (- 19m 59s) (3 60%) 1.1051\n",
      "30m 14s (- 20m 9s) (3 60%) 1.1768\n",
      "30m 30s (- 20m 20s) (3 60%) 1.1546\n",
      "30m 45s (- 20m 30s) (3 60%) 1.1885\n",
      "31m 0s (- 20m 40s) (3 60%) 1.1407\n",
      "31m 16s (- 20m 50s) (3 60%) 1.1991\n",
      "31m 31s (- 21m 0s) (3 60%) 1.1124\n",
      "31m 46s (- 21m 11s) (3 60%) 1.1559\n",
      "32m 1s (- 21m 21s) (3 60%) 1.1245\n",
      "32m 16s (- 21m 31s) (3 60%) 1.1363\n",
      "---------------------------------------------------------------------\n",
      "----------------Saving trained model---------------------------------\n",
      "---------------------------------------------------------------------\n",
      "32m 27s (- 8m 6s) (4 80%) 0.8400\n",
      "32m 43s (- 8m 10s) (4 80%) 1.0404\n",
      "32m 58s (- 8m 14s) (4 80%) 1.0477\n",
      "33m 13s (- 8m 18s) (4 80%) 1.0824\n",
      "33m 28s (- 8m 22s) (4 80%) 1.0983\n",
      "33m 43s (- 8m 25s) (4 80%) 1.0621\n",
      "33m 58s (- 8m 29s) (4 80%) 1.1448\n",
      "34m 14s (- 8m 33s) (4 80%) 1.1177\n",
      "34m 29s (- 8m 37s) (4 80%) 1.0940\n",
      "34m 44s (- 8m 41s) (4 80%) 1.1422\n",
      "34m 59s (- 8m 44s) (4 80%) 1.0629\n",
      "35m 14s (- 8m 48s) (4 80%) 1.0595\n",
      "35m 29s (- 8m 52s) (4 80%) 1.0620\n",
      "35m 45s (- 8m 56s) (4 80%) 1.1084\n",
      "36m 0s (- 9m 0s) (4 80%) 1.0975\n",
      "36m 15s (- 9m 3s) (4 80%) 1.1235\n",
      "36m 30s (- 9m 7s) (4 80%) 1.0404\n",
      "36m 45s (- 9m 11s) (4 80%) 1.0881\n",
      "37m 0s (- 9m 15s) (4 80%) 1.1519\n",
      "37m 15s (- 9m 18s) (4 80%) 1.0785\n",
      "37m 31s (- 9m 22s) (4 80%) 1.1498\n",
      "37m 46s (- 9m 26s) (4 80%) 1.1390\n",
      "38m 1s (- 9m 30s) (4 80%) 1.1347\n",
      "38m 16s (- 9m 34s) (4 80%) 1.1346\n",
      "38m 31s (- 9m 37s) (4 80%) 1.1598\n",
      "38m 47s (- 9m 41s) (4 80%) 1.1074\n",
      "39m 2s (- 9m 45s) (4 80%) 1.1592\n",
      "39m 17s (- 9m 49s) (4 80%) 1.1459\n",
      "39m 32s (- 9m 53s) (4 80%) 1.0358\n",
      "39m 47s (- 9m 56s) (4 80%) 1.1165\n",
      "40m 2s (- 10m 0s) (4 80%) 1.1163\n",
      "40m 18s (- 10m 4s) (4 80%) 1.1154\n",
      "40m 33s (- 10m 8s) (4 80%) 1.0590\n",
      "40m 48s (- 10m 12s) (4 80%) 1.1352\n",
      "41m 3s (- 10m 15s) (4 80%) 1.1153\n",
      "41m 18s (- 10m 19s) (4 80%) 1.1561\n",
      "41m 33s (- 10m 23s) (4 80%) 1.1328\n",
      "41m 48s (- 10m 27s) (4 80%) 1.1178\n",
      "42m 4s (- 10m 31s) (4 80%) 1.1485\n",
      "42m 19s (- 10m 34s) (4 80%) 1.0622\n",
      "42m 34s (- 10m 38s) (4 80%) 1.1191\n",
      "42m 49s (- 10m 42s) (4 80%) 1.0977\n",
      "43m 4s (- 10m 46s) (4 80%) 1.1861\n",
      "---------------------------------------------------------------------\n",
      "----------------Saving trained model---------------------------------\n",
      "---------------------------------------------------------------------\n",
      "43m 15s (- 0m 0s) (5 100%) 0.7812\n",
      "43m 30s (- 0m 0s) (5 100%) 1.0358\n",
      "43m 45s (- 0m 0s) (5 100%) 0.9991\n",
      "44m 0s (- 0m 0s) (5 100%) 1.0968\n",
      "44m 16s (- 0m 0s) (5 100%) 1.0771\n",
      "44m 31s (- 0m 0s) (5 100%) 1.0564\n",
      "44m 46s (- 0m 0s) (5 100%) 1.1509\n",
      "45m 1s (- 0m 0s) (5 100%) 1.0345\n",
      "45m 16s (- 0m 0s) (5 100%) 1.1388\n",
      "45m 31s (- 0m 0s) (5 100%) 1.0749\n",
      "45m 47s (- 0m 0s) (5 100%) 1.1467\n",
      "46m 2s (- 0m 0s) (5 100%) 1.0879\n",
      "46m 17s (- 0m 0s) (5 100%) 1.1947\n",
      "46m 32s (- 0m 0s) (5 100%) 1.0824\n",
      "46m 47s (- 0m 0s) (5 100%) 1.0669\n",
      "47m 3s (- 0m 0s) (5 100%) 1.1187\n",
      "47m 18s (- 0m 0s) (5 100%) 1.0935\n",
      "47m 33s (- 0m 0s) (5 100%) 1.1381\n",
      "47m 48s (- 0m 0s) (5 100%) 1.0236\n",
      "48m 3s (- 0m 0s) (5 100%) 1.1204\n",
      "48m 18s (- 0m 0s) (5 100%) 1.1276\n",
      "48m 34s (- 0m 0s) (5 100%) 1.1138\n",
      "48m 49s (- 0m 0s) (5 100%) 1.1094\n",
      "49m 4s (- 0m 0s) (5 100%) 1.1269\n",
      "49m 19s (- 0m 0s) (5 100%) 1.0089\n",
      "49m 34s (- 0m 0s) (5 100%) 1.1096\n",
      "49m 49s (- 0m 0s) (5 100%) 1.1589\n",
      "50m 4s (- 0m 0s) (5 100%) 1.0785\n",
      "50m 20s (- 0m 0s) (5 100%) 1.0955\n",
      "50m 35s (- 0m 0s) (5 100%) 1.0889\n",
      "50m 50s (- 0m 0s) (5 100%) 1.0411\n",
      "51m 5s (- 0m 0s) (5 100%) 1.0686\n",
      "51m 20s (- 0m 0s) (5 100%) 1.1214\n",
      "51m 35s (- 0m 0s) (5 100%) 1.1098\n",
      "51m 50s (- 0m 0s) (5 100%) 1.0642\n",
      "52m 5s (- 0m 0s) (5 100%) 1.1144\n",
      "52m 21s (- 0m 0s) (5 100%) 1.0743\n",
      "52m 36s (- 0m 0s) (5 100%) 1.1651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52m 51s (- 0m 0s) (5 100%) 1.1719\n",
      "53m 6s (- 0m 0s) (5 100%) 1.0815\n",
      "53m 22s (- 0m 0s) (5 100%) 1.1529\n",
      "53m 36s (- 0m 0s) (5 100%) 1.0191\n",
      "53m 52s (- 0m 0s) (5 100%) 1.1317\n",
      "---------------------------------------------------------------------\n",
      "----------------Saving trained model---------------------------------\n",
      "---------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.014513195508146939,\n",
       " 1.183895236498689,\n",
       " 1.1180961216965766,\n",
       " 1.1673353513952802,\n",
       " 1.1693951551881556,\n",
       " 1.194572694856827,\n",
       " 1.1577581818463052,\n",
       " 1.1753352711298692,\n",
       " 1.1793689163417032,\n",
       " 1.1362938779020966,\n",
       " 1.1469792729207915,\n",
       " 1.2082777639937723,\n",
       " 1.1140832498628799,\n",
       " 1.178837338277738,\n",
       " 1.1493046528019315,\n",
       " 1.1720584785774966,\n",
       " 1.1693237158370349,\n",
       " 1.1662964363620707,\n",
       " 1.2377224700091636,\n",
       " 1.1736664952317326,\n",
       " 1.1902935315484873,\n",
       " 1.1739769337275252,\n",
       " 0.40713197420721187,\n",
       " 1.0818044782664678,\n",
       " 1.1357374280119596,\n",
       " 1.102879790214643,\n",
       " 1.183231189152966,\n",
       " 1.1404686559389714,\n",
       " 1.1763945676202643,\n",
       " 1.1526039635645204,\n",
       " 1.0922105219592797,\n",
       " 1.129868663108512,\n",
       " 1.1364962664042433,\n",
       " 1.1034779070501457,\n",
       " 1.143943790279023,\n",
       " 1.1491523983053964,\n",
       " 1.0917859602627686,\n",
       " 1.1859824867771094,\n",
       " 1.1596516371426515,\n",
       " 1.1788473364425027,\n",
       " 1.1564851050180933,\n",
       " 1.1377085902592905,\n",
       " 1.1733535082046305,\n",
       " 1.1519726024261894,\n",
       " 0.39225625077339066,\n",
       " 1.1342568818183794,\n",
       " 1.097909052600599,\n",
       " 1.1031384455014581,\n",
       " 1.113788509630177,\n",
       " 1.0917102400897298,\n",
       " 1.1496238583081393,\n",
       " 1.1013882482868351,\n",
       " 1.1276294170013845,\n",
       " 1.1485676673993677,\n",
       " 1.1113665687874572,\n",
       " 1.120517215467479,\n",
       " 1.138138823835817,\n",
       " 1.1357176830344007,\n",
       " 1.138804634825824,\n",
       " 1.1338461350741451,\n",
       " 1.108874510673628,\n",
       " 1.1409434441344382,\n",
       " 1.1715117342178136,\n",
       " 1.1699069631916206,\n",
       " 1.1341383487231114,\n",
       " 1.1304382831103186,\n",
       " 0.4200180774845489,\n",
       " 1.0440466366075491,\n",
       " 1.0903361213370544,\n",
       " 1.103452991851389,\n",
       " 1.1058871224808364,\n",
       " 1.1025719023404055,\n",
       " 1.0607817124667231,\n",
       " 1.1029761750730747,\n",
       " 1.0819442101047458,\n",
       " 1.1200212598826789,\n",
       " 1.114150733947754,\n",
       " 1.136826718474088,\n",
       " 1.147227703773812,\n",
       " 1.1332916557625545,\n",
       " 1.0908644913973875,\n",
       " 1.1163649357834913,\n",
       " 1.0872419858958615,\n",
       " 1.1252490260503065,\n",
       " 1.1444060432747616,\n",
       " 1.1331617799523759,\n",
       " 1.0906521235426814,\n",
       " 1.141877664017351,\n",
       " 0.3906215317608558,\n",
       " 1.0174470060165617,\n",
       " 1.0869366162443814,\n",
       " 1.1036496170252972,\n",
       " 1.0866842766330662,\n",
       " 1.1107993149430784,\n",
       " 1.1412726253352754,\n",
       " 1.0746392001844434,\n",
       " 1.1061053764656794,\n",
       " 1.080856571458791,\n",
       " 1.1239692405805197,\n",
       " 1.1115689327292244,\n",
       " 1.067862365409119,\n",
       " 1.1342635841892188,\n",
       " 1.087013932319536,\n",
       " 1.0650383356172746,\n",
       " 1.09504188694366,\n",
       " 1.0870376466724967,\n",
       " 1.094343606086626,\n",
       " 1.1684830563688933,\n",
       " 1.1171710602224687,\n",
       " 1.0754031152594576]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "encoder1 = EncoderRNN(EMBEDDING_SIZE,hidden_size).to(device)\n",
    "decoder1 = AttnDecoderRNN(EMBEDDING_SIZE,hidden_size, len(ordered_words_ft)).to(device)\n",
    "\n",
    "##UNCOMMENT TO TRAIN THE MODEL\n",
    "trainIters(encoder1, decoder1, 5, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the best models\n",
    "hidden_size=300\n",
    "\n",
    "folder = './attentation_model'\t\n",
    "\n",
    "encoder2 = EncoderRNN(EMBEDDING_SIZE,hidden_size).to(device)\n",
    "decoder2 = AttnDecoderRNN(EMBEDDING_SIZE,hidden_size, len(ordered_words_ft)).to(device)\n",
    "\n",
    "encoder2.load_state_dict(torch.load(folder+\"/Encoder_b\"))\n",
    "decoder2.load_state_dict(torch.load(folder+\"/Decoder_b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader can be test_loader or val_loader\n",
    "def evaluate(loader, encoder, decoder, beam = False, beam_k = 1):\n",
    "    bleu_score_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data_s1, data_s2, lengths_s1, lengths_s2) in enumerate(loader):\n",
    "            input_tensor = data_s1\n",
    "            input_length = input_tensor.size()[0]\n",
    "            #sentence_length to the output length\n",
    "            sentence_length = data_s2.size()[1]\n",
    "            encoder_hidden = encoder.initHidden(input_length)\n",
    "\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "            \n",
    "            #decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "            decoder_input = torch.tensor(np.array([[SOS_IDX]]*input_length).reshape(1,input_length),device=device)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoder_attentions = torch.zeros(sentence_length, sentence_length)\n",
    "            decoded_words_eval = []\n",
    "            sequences = [[list(), 1.0]]*input_length\n",
    "            for di in range(sentence_length):\n",
    "                decoded_words_sub = []\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_output)\n",
    "                # decoder_attentions[di] = decoder_attention.data\n",
    "                # topk(1) - softmax probability maximum\n",
    "                if beam == True:\n",
    "                    pass\n",
    "#                     topv, topi = decoder_output.data.topk(beam_k)\n",
    "#                     #batch loop\n",
    "#                     C = []\n",
    "#                     for idx, ind in enumerate(topi):\n",
    "#                         H, _ = sequences[idx]\n",
    "#                         for ele in ind:\n",
    "#                             if ele.item() == EOS_IDX:\n",
    "#                                 H.append('<EOS>')\n",
    "#                             else:\n",
    "#                                 H.append(idx2words_ft_en[ele.item()])\n",
    "                         \n",
    "                else:\n",
    "                    topv, topi = decoder_output.data.topk(1) \n",
    "                #batch loop\n",
    "                \n",
    "                eos_flag = False\n",
    "                for ind in topi:\n",
    "                    if eos_flag == False:\n",
    "                        if ind.item() == EOS_IDX:\n",
    "                            decoded_words_sub.append('</s>')\n",
    "                            eos_flag = True\n",
    "                            #break\n",
    "                        else:\n",
    "                            decoded_words_sub.append(idx2words_ft_en[ind.item()])\n",
    "                    else:\n",
    "                        decoded_words_sub.append(\"<pad>\")\n",
    "                        \n",
    "                \n",
    "                decoded_words_eval.append(decoded_words_sub)\n",
    "                \n",
    "                #swap dimensions of decoded_words to [batch_size * 377]\n",
    "                \n",
    "                #decoded_words_new = [[i for i in ele] for ele in list(zip(*decoded_words_eval))]\n",
    "\n",
    "                #change the dimension\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                decoder_input = decoder_input.unsqueeze(0)\n",
    "            \n",
    "            \n",
    "            pred_num = 0\n",
    "            listed_predictions = []\n",
    "            \n",
    "            \n",
    "            decoded_words_new = [[i for i in ele] for ele in list(zip(*decoded_words_eval))]\n",
    "            for token_list in decoded_words_new:\n",
    "                sent = ' '.join(str(token) for token in token_list if token!=\"<pad>\")\n",
    "                #print (sent)\n",
    "                listed_predictions.append(sent)\n",
    "                pred_num += 1\n",
    "                \n",
    "            ref_num = 0\n",
    "            listed_reference = []\n",
    "            for ele in data_s2:\n",
    "                sent = index2token_sentence(ele)\n",
    "                #print (tokens)\n",
    "                #sent = ' '.join(tokens)\n",
    "                #print (sent)\n",
    "                listed_reference.append(sent)\n",
    "                ref_num += 1\n",
    "            \n",
    "            \n",
    "            bleu_score = corpus_bleu(listed_predictions,[listed_reference])\n",
    "            \n",
    "            #uncommon to print prediction and reference\n",
    "            #print (listed_predictions)\n",
    "            #print (listed_reference)\n",
    "            \n",
    "            \n",
    "            print('BLEU Score is %s' % (str(bleu_score.score)))\n",
    "           \n",
    "        bleu_score_list.append(bleu_score)\n",
    "        return bleu_score_list, decoded_words_new, decoder_attentions[:di + 1]\n",
    "    \n",
    "def index2token_batch(list_of_list):\n",
    "    return ' '.join(idx2words_ft_en[r.item()] for v in list_of_list for r in v if r.item()!=PAD_IDX)\n",
    "def index2token_sentence(sentence_batch):\n",
    "    return ' '.join(idx2words_ft_en[sent.item()] for sent in sentence_batch if sent.item()!=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score is 20.099632842263766\n",
      "BLEU Score is 17.2331000405467\n",
      "BLEU Score is 18.263529593949873\n",
      "BLEU Score is 18.156729843585964\n",
      "BLEU Score is 16.248083583710795\n",
      "BLEU Score is 16.6067262093732\n",
      "BLEU Score is 23.67949056871905\n",
      "BLEU Score is 17.897392446235028\n",
      "BLEU Score is 20.546576246012076\n",
      "BLEU Score is 20.05563999110098\n",
      "BLEU Score is 16.732889645990205\n",
      "BLEU Score is 16.704218238382563\n",
      "BLEU Score is 18.362462385489334\n",
      "BLEU Score is 20.55813622830437\n"
     ]
    }
   ],
   "source": [
    "blue, output_words, attentions = evaluate(val_loader,\n",
    "    encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
