{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "Add code to save graphs in showPlot function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re  \n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "from itertools import dropwhile\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install sacrebleu\n",
    "from sacrebleu import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in chinese-english pairs\n",
    "#read in chinese-english pairs\n",
    "lines_zh = open(PATH+'iwslt-zh-en/train.tok.zh',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en = open(PATH+'iwslt-zh-en/train.tok.en',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_zh_test = open(PATH+'iwslt-zh-en/test.tok.zh',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_test = open(PATH+'iwslt-zh-en/test.tok.en',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_zh_val = open(PATH+'iwslt-zh-en/dev.tok.zh',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_val = open(PATH+'iwslt-zh-en/dev.tok.en',encoding = 'utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delect_least_common_words(list_sent, threshold = 5):\n",
    "    ret_list =[]\n",
    "    for x in list_sent:\n",
    "        ret_list += x.split()\n",
    "    ret_dic = collections.Counter(ret_list)\n",
    "    \n",
    "    #print (ret_dic[\"&amp;\"])\n",
    "    #print (ret_dic[\"&apos;\"])\n",
    "    #print (ret_dic[\"&quot;\"])\n",
    "    #print (ret_dic[\"&#91\"])\n",
    "    for key, count in dropwhile(lambda key_count: key_count[1] >= threshold, ret_dic.most_common()):\n",
    "        \n",
    "        del ret_dic[key]\n",
    "        \n",
    "        \n",
    "    return list(ret_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_words = delect_least_common_words(lines_zh)\n",
    "en_words = delect_least_common_words(lines_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34443"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zh_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_load = 100000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "\n",
    "with open(PATH+'cc.zh.300.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+3, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    ordered_words_ft.extend(['<pad>', '<unk>', '<s>'])\n",
    "    loaded_embeddings_ft[0,:] = np.zeros(300)\n",
    "    loaded_embeddings_ft[1,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft[2,:] = np.random.normal(size = 300)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+3, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+3\n",
    "        idx2words_ft[i+3] = s[0]\n",
    "        ordered_words_ft.append(s[0])\n",
    "    length = len(np.setdiff1d(zh_words, ordered_words_ft))\n",
    "    tmp_embeddings = np.zeros((length, 300))\n",
    "    for idx, word in enumerate(np.setdiff1d(zh_words, ordered_words_ft)):\n",
    "        words_ft[word] = idx+words_to_load+3\n",
    "        idx2words_ft[idx+words_to_load+3] = word\n",
    "        tmp_embeddings[idx, :] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft = np.concatenate((loaded_embeddings_ft, tmp_embeddings), axis = 0)\n",
    "    words_ft['<pad>'] = PAD_IDX\n",
    "    words_ft['<unk>'] = UNK_IDX\n",
    "    words_ft['<s>'] = SOS_IDX\n",
    "    idx2words_ft[PAD_IDX] = '<pad>'\n",
    "    idx2words_ft[UNK_IDX] = '<unk>'\n",
    "    idx2words_ft[SOS_IDX] = '<s>'\n",
    "    \n",
    "ordered_words_ft = list(words_ft.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English embedding\n",
    "with open(PATH+'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft_en = np.zeros((words_to_load+4, 300))\n",
    "    words_ft_en = {}\n",
    "    idx2words_ft_en = {}\n",
    "    ordered_words_ft_en = []\n",
    "    ordered_words_ft_en.extend(['<pad>', '<unk>', '<s>', '</s>'])\n",
    "    loaded_embeddings_ft_en[0,:] = np.zeros(300)\n",
    "    loaded_embeddings_ft_en[1,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en[2,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en[3,:] = np.random.normal(size = 300)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft_en[i+4, :] = np.asarray(s[1:])\n",
    "        words_ft_en[s[0]] = i+4\n",
    "        idx2words_ft_en[i+4] = s[0]\n",
    "        ordered_words_ft_en.append(s[0])\n",
    "    length = len(np.setdiff1d(en_words, ordered_words_ft_en))\n",
    "    tmp_embeddings = np.zeros((length, 300))\n",
    "    for idx, word in enumerate(np.setdiff1d(en_words, ordered_words_ft_en)):\n",
    "        words_ft_en[word] = idx+words_to_load+4\n",
    "        idx2words_ft_en[idx+words_to_load+4] = word\n",
    "        tmp_embeddings[idx, :] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en = np.concatenate((loaded_embeddings_ft_en, tmp_embeddings), axis = 0)\n",
    "    words_ft_en['<pad>'] = PAD_IDX\n",
    "    words_ft_en['<unk>'] = UNK_IDX\n",
    "    words_ft_en['<s>'] = SOS_IDX\n",
    "    words_ft_en['</s>'] = EOS_IDX\n",
    "    idx2words_ft_en[PAD_IDX] = '<pad>'\n",
    "    idx2words_ft_en[UNK_IDX] = '<unk>'\n",
    "    idx2words_ft_en[SOS_IDX] = '<s>'\n",
    "    idx2words_ft_en[EOS_IDX] = '</s>'\n",
    "    \n",
    "ordered_words_ft_en = list(words_ft_en.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(idx2words_ft) == len(words_ft)\n",
    "assert len(loaded_embeddings_ft) == len(words_ft)\n",
    "assert len(idx2words_ft_en) == len(words_ft_en)\n",
    "assert len(loaded_embeddings_ft_en) == len(words_ft_en)\n",
    "assert len(ordered_words_ft_en) == len(loaded_embeddings_ft_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add sos and eos in each sentence\n",
    "def add_sos_eos(lines):\n",
    "    \n",
    "    train = []\n",
    "    for l in lines:\n",
    "        l = '<s> ' + l + ' </s>'\n",
    "        train.append(l)\n",
    "    return train\n",
    "zh_train = add_sos_eos(lines_zh)    \n",
    "en_train = add_sos_eos(lines_en)\n",
    "zh_test = add_sos_eos(lines_zh_test)\n",
    "en_test = add_sos_eos(lines_en_test)\n",
    "zh_val = add_sos_eos(lines_zh_val)\n",
    "en_val = add_sos_eos(lines_en_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> And the problem , I think , is that we take the ocean for granted . </s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data,eng = False):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = []\n",
    "        for token in tokens.split():\n",
    "            if eng == False:\n",
    "                try:\n",
    "                    index_list.append(words_ft[token])\n",
    "                except KeyError:\n",
    "                    index_list.append(UNK_IDX)\n",
    "            else:\n",
    "                try:\n",
    "                    index_list.append(words_ft_en[token])\n",
    "                except KeyError:\n",
    "                    index_list.append(UNK_IDX)\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_train_indices = token2index_dataset(zh_train)\n",
    "en_train_indices = token2index_dataset(en_train,eng = True)\n",
    "zh_val_indices = token2index_dataset(zh_val)\n",
    "en_val_indices = token2index_dataset(en_val,eng = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_sentence_length\n",
    "length_of_en = [len(x.split()) for x in en_train]\n",
    "max_sentence_length_en = sorted(length_of_en)[-int(len(length_of_en)*0.01)]\n",
    "length_of_zh = [len(x.split()) for x in zh_train]\n",
    "max_sentence_length_zh = sorted(length_of_zh)[-int(len(length_of_zh)*0.01)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Data Loader\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class load_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list_s1,data_list_s2):\n",
    "        \"\"\"\n",
    "        @param data_list_zh: list of Chinese tokens \n",
    "        @param data_list_en: list of English tokens as TARGETS\n",
    "        \"\"\"\n",
    "        self.data_list_s1 = data_list_s1\n",
    "        self.data_list_s2 = data_list_s2\n",
    "        \n",
    "        assert (len(self.data_list_s1) == len(self.data_list_s2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list_s1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx_s1 = self.data_list_s1[key][:max_sentence_length_zh]\n",
    "        token_idx_s2 = self.data_list_s2[key][:max_sentence_length_en]\n",
    "        return [token_idx_s1, token_idx_s2, len(token_idx_s1), len(token_idx_s2)]\n",
    "\n",
    "def collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list_s1 = []\n",
    "    data_list_s2 = []\n",
    "    length_list_s1 = []\n",
    "    length_list_s2 = []\n",
    "    for datum in batch:\n",
    "        length_list_s1.append(datum[2])\n",
    "        length_list_s2.append(datum[3])\n",
    "        padded_vec_zh = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,max_sentence_length_zh-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_en = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,max_sentence_length_en-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_s1.append(padded_vec_zh[:max_sentence_length_zh])\n",
    "        data_list_s2.append(padded_vec_en[:max_sentence_length_en])\n",
    "    #print(type(data_list_s1[0]))\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list_s1)).to(device), torch.from_numpy(np.array(data_list_s2)).to(device),\n",
    "            torch.LongTensor(length_list_s1).to(device), torch.LongTensor(length_list_s2).to(device)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "EMBEDDING_SIZE = 300 # fixed as from the input embedding data\n",
    "\n",
    "train_dataset = load_dataset(zh_train_indices, en_train_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = load_dataset(zh_val_indices, en_val_indices)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_size, embed= torch.from_numpy(loaded_embeddings_ft).float(),num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers \n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed, freeze=False)\n",
    "        #self.gru = nn.GRU(emb_dim, hidden_size,num_layers=num_layers,batch_first=True,bidirectional = True)\n",
    "        self.rnn = nn.LSTM(self.emb_dim, self.hidden_size, batch_first=True,\n",
    "                           num_layers=self.num_layers, bidirectional=True)\n",
    "\n",
    "    def forward(self, data, hidden):\n",
    "        #hidden is a tuple (h,c)\n",
    "        #dimension of h: num_layers * num_directions, batch, hidden_size \n",
    "        \n",
    "        batch_size, seq_len = data.size()\n",
    "        \n",
    "        embed = self.embedding(data)\n",
    "        \n",
    "        \n",
    "        #output, hidden = self.gru(embed,hidden)\n",
    "        \n",
    "        #hidden is a tuple (h,c). Dim of h: num_layers * num_directions, batch, hidden_size\n",
    "        output, (h,c) = self.rnn(embed,hidden)\n",
    "        \n",
    "        h = torch.sum(h, dim=0).unsqueeze(0)\n",
    "        c = torch.sum(c, dim=0).unsqueeze(0)\n",
    "        \n",
    "        hidden = (h,c)\n",
    "        \n",
    "        \n",
    "        output = (output[:, :, :self.hidden_size] +\n",
    "                output[:, :, self.hidden_size:])\n",
    "        \n",
    "        ## potentially there are other ways \n",
    "        \n",
    "        \n",
    "        #hidden = [n layers * n directions =1 , batch_size, hidden_size ]\n",
    "        #print (\"encoder hidden\",hidden)\n",
    "        #print (\"encoder output\", output.shape)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    # initialize the hidden with random numbers\n",
    "    def initHidden(self,batch_size):\n",
    "        return (torch.randn(2*self.num_layers, batch_size, self.hidden_size,device=device),\n",
    "                torch.randn(2*self.num_layers, batch_size, self.hidden_size,device=device))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,emb_dim,hidden_size, output_size, embed= torch.from_numpy(loaded_embeddings_ft_en).float(),num_layers=1,\n",
    "                 dropout_p=0.1, max_length=max_sentence_length_zh):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "       \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers \n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed, freeze=False)\n",
    "        self.attn = nn.Linear(self.hidden_size + emb_dim, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size + emb_dim, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        #self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, bidirectional = False)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, data, hidden,encoder_outputs):\n",
    "        \n",
    "        ### embed: [1 * batch size * emb_dim = 300 ] ###\n",
    "        ### hidden: [1 * batch size * hidden_size = 300 ] ###\n",
    "        ### FOR LSTM, HIDDEN: tuple (h,c). h:(2, batch size, hidden_size)\n",
    "        ### encoder_outputs: [batch size * max_sentence_length_zh * hidden_size = 300 ] ###\n",
    "        ### 因为这里concat之后，attn layer 他给的是 hidden size *2 \n",
    "        ### 所以我这儿的hidden size就只能写300了 \n",
    "        \n",
    "        embed = self.embedding(data)\n",
    "        embed = self.dropout(embed)    \n",
    "        ### torch.cat((embed, hidden), 2)  \n",
    "        ### [1 * batch size * (emb_dim + hidden_size) ]\n",
    "        \n",
    "        ### attn_weights: [1 * batch size * max_sentence_length_zh ]###\n",
    "        ### attn_weights[0].unsqueeze(1): [batch size * 1 * max_sentence_length_zh ]###\n",
    "        \n",
    "        ### softmax dim=2 因为最后一个dimension是 词组什么的，不能是1，1的话就是\n",
    "        ### 不同batch间这样比较了？\n",
    "        \n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embed, hidden[0]), 2)), dim=2)\n",
    "        \n",
    "\n",
    "        ### torch.bmm(attn_weights[0].unsqueeze(1),encoder_outputs).squeeze(1) :\n",
    "        ### [batch size * 1 * hidden_size ]###\n",
    "\n",
    "        ### attn_applied: [batch size * hidden_size (= 300) ] ###\n",
    "     \n",
    "        attn_applied = torch.bmm(attn_weights[0].unsqueeze(1),\n",
    "                                 encoder_outputs).squeeze(1)\n",
    "        \n",
    "        ### output: [batch size * hidden_size (= 300) ] ###\n",
    "        ### embed[0]: [batch size * hidden_size (= 300) ] ###\n",
    "\n",
    "        output = torch.cat((embed[0], attn_applied), 1)\n",
    " \n",
    "        ### output: [1 * batch size * hidden_size (= 300) ] ###\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        ### output: [1 * batch size * hidden_size (= 300) ] ###\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #output, hidden = self.gru(output, hidden)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return torch.randn(self.num_layers, batch_size, self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1\n",
    "#input_tensor: list of sentence tensor\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "          criterion, eee):\n",
    "    \n",
    "    ### target_tensor [batch size, max_sentence_length_en = 377] ###\n",
    "    ### target_tensor [batch size, max_sentence_length_zh = 220] ###\n",
    "    batch_size_1, input_length = input_tensor.size()\n",
    "    batch_size_2, target_length = target_tensor.size()\n",
    "    #print (\"target length \", target_length)\n",
    "    \n",
    "    \n",
    "    encoder_hidden = encoder.initHidden(batch_size_1)\n",
    "    h,c = encoder_hidden\n",
    "    #print (\"encoder hidden init, \",h.shape)\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "    ### encoder_hidden: 1 * batch * hidden size ### \n",
    "    ### encoder_output: batch size * max_sentence_length_zh * hidden size ### \n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "    #print (\"encoder output, \", encoder_output.shape)\n",
    "\n",
    "    decoder_input = torch.tensor(np.array([[SOS_IDX]]*batch_size_1).reshape(1,batch_size_1),device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    h1,c1 = encoder_hidden\n",
    "    #print (\"encoder hidden,\", h1.shape)\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    #print(use_teacher_forcing)\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            \n",
    "            ### decoder_output: [batchsize,5000] ###\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden,encoder_output)\n",
    "            \n",
    "            #print (\"decoder output, \",decoder_output.shape)\n",
    "            #print (\"target_tensor, \",len(target_tensor[:,di]))\n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            decoder_input = target_tensor[:,di].unsqueeze(0)  # Teacher forcing\n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden,encoder_output)\n",
    "                        \n",
    "            ### decoder_output [batch size, 50003]  ###\n",
    "            \n",
    "            ### topi is a [batch size, 1] tensor first we remove the size 1\n",
    "            ### demension then we add it at the beginning using squeeze\n",
    "            ### 有点脑残诶，做个转置不就好了？\n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            \n",
    "            ### decoder_input [1, batch size]  ###\n",
    "            decoder_input = decoder_input.unsqueeze(0)\n",
    " \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, folder,print_every=1, plot_every=100, evaluate_every = 50,read_in_model = False,learning_rate=0.001,early_stop_tol = 10e-7):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    plot_val = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    patience = 0\n",
    "    \n",
    "    early_stopped = False\n",
    "    current_best_bleu = 0\n",
    "    \n",
    "    best_encoder = encoder.state_dict()\n",
    "    best_decoder = decoder.state_dict()\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------\t\n",
    "    #\t\n",
    "    #    LOAD MODELS\t\n",
    "    #\t\n",
    "    #--------------------------------------------\t\n",
    "    \t\n",
    "        \n",
    "    \n",
    "    if not os.path.exists(folder):\t\n",
    "        os.makedirs(folder)\t\n",
    "\n",
    "    if read_in_model == True:\n",
    "        if os.path.exists(folder+'/Encoder'):\t\n",
    "            print('---------------------------------------------------------------------')\t\n",
    "            print('----------------Readind trained model---------------------------------')\t\n",
    "            print('---------------------------------------------------------------------')\t\n",
    "\n",
    "            #read trained models\t\n",
    "            encoder.load_state_dict(torch.load(folder+\"/Encoder\"))\n",
    "            decoder.load_state_dict(torch.load(folder+\"/Decoder\"))\t\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #criterion_val = nn.CrossEntropyLoss()\n",
    "    \n",
    "    last_val = 0\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        \n",
    "        for i, (data_s1, data_s2, lengths_s1, lengths_s2) in enumerate(train_loader):\n",
    "            input_tensor = data_s1\n",
    "            target_tensor = data_s2\n",
    "            #print(\"train\",target_tensor.size())\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion,i)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                if i != 0:\n",
    "                    print_loss_avg = print_loss_total / print_every\n",
    "                    print_loss_total = 0\n",
    "                    print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                 iter, iter / n_iters * 100, print_loss_avg))\n",
    "                \n",
    "            if i % plot_every == 0:\n",
    "                if i != 0:\n",
    "                    plot_loss_avg = plot_loss_total / plot_every\n",
    "                    plot_losses.append(plot_loss_avg)\n",
    "                    plot_loss_total = 0\n",
    "                \n",
    "            if i % evaluate_every == 0:\n",
    "                if i != 0:\n",
    "                    bleu_score,output_words,attentions = evaluate(val_loader, encoder, decoder)\n",
    "                    if bleu_score > current_best_bleu:\n",
    "                        current_best_bleu = bleu_score\n",
    "                        \n",
    "                        best_encoder = encoder.state_dict()\n",
    "                        best_decoder = decoder.state_dict()\n",
    "                        \n",
    "                    plot_val.append(bleu_score)\n",
    "                    #print (\"BLEU: \",bleu_score)\n",
    "                    \n",
    "                    if bleu_score <= current_best_bleu:\n",
    "                        patience += 1\n",
    "                        \n",
    "                    elif bleu_score > current_best_bleu and np.abs(bleu_score - current_best_bleu)/float(current_best_bleu) < early_stop_tol:\n",
    "                        patience += 1\n",
    "                    \n",
    "                    else:\n",
    "                        patience = 0\n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                    \"\"\"\n",
    "                    #If new bleu score is lower than last time\n",
    "                    if bleu_score <= last_val:\n",
    "                        patience += 1\n",
    "                    #or does not improve by enough percentage\n",
    "                    elif bleu_score > last_val and np.abs(bleu_score - last_val)/float(last_val) < early_stop_tol:\n",
    "                        \n",
    "                        patience += 1\n",
    "                    #bleu score increased since last time\n",
    "                    else:\n",
    "                        #reset patience\n",
    "                        patience = 0\n",
    "                            \n",
    "                    \"\"\"    \n",
    "                    if patience == 10:\n",
    "                       \n",
    "                        torch.save(best_encoder,folder +\"/Encoder\")\n",
    "                        torch.save(best_decoder,folder +\"/Decoder\")\n",
    "                        early_stopped = True\n",
    "                        patience = 0\n",
    "            \n",
    "                        \n",
    "                    last_val = bleu_score\n",
    "                 \n",
    "        if early_stopped == False:\n",
    "        \n",
    "            # Save the model for every epoch\n",
    "            print('---------------------------------------------------------------------')\t\n",
    "            print('----------------Saving trained model---------------------------------')\t\n",
    "            print('---------------------------------------------------------------------')\t\n",
    "\n",
    "            torch.save(encoder.state_dict(),folder +\"/Encoder\")\n",
    "            torch.save(decoder.state_dict(),folder +\"/Decoder\")\n",
    "\n",
    "    showPlot(plot_losses,title = \"Train Loss\",name = \"loss_\"+folder)\n",
    "    showPlot(plot_val, title = \"BLEU Score on Validation Set\",name = \"bleu_\"+folder)\n",
    "    return plot_losses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points,title,name):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    #loc = ticker.MultipleLocator(base=0.2)\n",
    "    #ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.title(title)\n",
    "    plt.save(name)\n",
    "    \n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader can be test_loader or val_loader\n",
    "def evaluate(loader, encoder, decoder, after_train_mode = False,beam = False, beam_k = 1):\n",
    "    bleu_score_list = []\n",
    "    big_pred_list = []\n",
    "    big_ref_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data_s1, data_s2, lengths_s1, lengths_s2) in enumerate(loader):\n",
    "            input_tensor = data_s1\n",
    "            input_length = input_tensor.size()[0]\n",
    "            #sentence_length to the output length\n",
    "            sentence_length = data_s2.size()[1]\n",
    "            encoder_hidden = encoder.initHidden(input_length)\n",
    "\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "            \n",
    "            #decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "            decoder_input = torch.tensor(np.array([[SOS_IDX]]*input_length).reshape(1,input_length),device=device)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoder_attentions = torch.zeros(sentence_length, sentence_length)\n",
    "            decoded_words_eval = []\n",
    "            sequences = [[list(), 1.0]]*input_length\n",
    "            for di in range(sentence_length):\n",
    "                decoded_words_sub = []\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_output)\n",
    "                # decoder_attentions[di] = decoder_attention.data\n",
    "                # topk(1) - softmax probability maximum\n",
    "                if beam == True:\n",
    "                    pass\n",
    "#                     topv, topi = decoder_output.data.topk(beam_k)\n",
    "#                     #batch loop\n",
    "#                     C = []\n",
    "#                     for idx, ind in enumerate(topi):\n",
    "#                         H, _ = sequences[idx]\n",
    "#                         for ele in ind:\n",
    "#                             if ele.item() == EOS_IDX:\n",
    "#                                 H.append('<EOS>')\n",
    "#                             else:\n",
    "#                                 H.append(idx2words_ft_en[ele.item()])\n",
    "                         \n",
    "                else:\n",
    "                    topv, topi = decoder_output.data.topk(1) \n",
    "                    \n",
    "                #batch loop\n",
    "                \n",
    "                \n",
    "                for ind in topi:\n",
    "                    \n",
    "                    if ind.item() == EOS_IDX:\n",
    "                        \n",
    "                        decoded_words_sub.append(idx2words_ft_en[EOS_IDX])\n",
    "                        \n",
    "                    else:\n",
    "                        decoded_words_sub.append(idx2words_ft_en[ind.item()])\n",
    "                    \n",
    "                \n",
    "                decoded_words_eval.append(decoded_words_sub)\n",
    "                \n",
    "                #swap dimensions of decoded_words to [batch_size * 377]\n",
    "                \n",
    "                #decoded_words_new = [[i for i in ele] for ele in list(zip(*decoded_words_eval))]\n",
    "\n",
    "                #change the dimension\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                decoder_input = decoder_input.unsqueeze(0)\n",
    "            \n",
    "            \n",
    "            pred_num = 0\n",
    "            listed_predictions = []\n",
    "            \n",
    "            \n",
    "            decoded_words_new = [[i for i in ele] for ele in list(zip(*decoded_words_eval))]\n",
    "            \n",
    "            for token_list in decoded_words_new:\n",
    "                sent = ' '.join(str(token) for token in token_list if token!=\"<pad>\" and token!=\"<s>\" and token!=\"</s>\")\n",
    "                #print (sent)\n",
    "                listed_predictions.append(sent)\n",
    "                #print (sent)\n",
    "                pred_num += 1\n",
    "                \n",
    "            ref_num = 0\n",
    "            listed_reference = []\n",
    "            for ele in data_s2:\n",
    "                sent = index2token_sentence(ele)\n",
    "                #print (tokens)\n",
    "                #sent = ' '.join(tokens)\n",
    "                #print (sent)\n",
    "                listed_reference.append(sent)\n",
    "                ref_num += 1\n",
    "            \n",
    "            big_pred_list += listed_predictions\n",
    "            big_ref_list += listed_reference\n",
    "            \n",
    "            assert len(big_pred_list) == len(big_ref_list)\n",
    "            \n",
    "            \n",
    "            #uncommon to print prediction and reference\n",
    "            #print (listed_predictions)\n",
    "            #print (listed_reference)\n",
    "        bleu_score = corpus_bleu(big_pred_list,[big_ref_list]).score\n",
    "        \n",
    "        if after_train_mode == True:\n",
    "            for idx,ele in enumerate(big_pred_list):\n",
    "                print (ele)\n",
    "                print (big_ref_list[idx])\n",
    "                print (\"\\n\")\n",
    "                \n",
    "                \n",
    "    print('BLEU Score is %s' % (str(bleu_score)))\n",
    "        \n",
    "\n",
    "    return bleu_score, decoded_words_new, decoder_attentions[:di + 1]\n",
    "    \n",
    "def index2token_batch(list_of_list):\n",
    "    return ' '.join(idx2words_ft_en[r.item()] for v in list_of_list for r in v if r.item()!=PAD_IDX)\n",
    "def index2token_sentence(sentence_batch):\n",
    "    return ' '.join(idx2words_ft_en[sent.item()] for sent in sentence_batch if sent.item()!=PAD_IDX and sent.item()!=SOS_IDX and sent.item()!=EOS_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-14766dec781a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mafter_train_mode\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder1' is not defined"
     ]
    }
   ],
   "source": [
    "score_list, output_words, attentions = evaluate(val_loader, encoder1, decoder1,after_train_mode =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 49s (- 3m 17s) (1 20%) 3.8770\n",
      "1m 37s (- 6m 31s) (1 20%) 2.0995\n",
      "2m 26s (- 9m 45s) (1 20%) 1.9332\n",
      "3m 14s (- 12m 58s) (1 20%) 1.8768\n",
      "4m 3s (- 16m 12s) (1 20%) 1.8637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score is 0.06972187215768907\n",
      "5m 0s (- 20m 1s) (1 20%) 1.8355\n",
      "5m 48s (- 23m 14s) (1 20%) 1.8346\n",
      "6m 37s (- 26m 28s) (1 20%) 1.7792\n",
      "7m 25s (- 29m 42s) (1 20%) 1.7647\n",
      "8m 14s (- 32m 56s) (1 20%) 1.7541\n",
      "BLEU Score is 0.5460177079688824\n",
      "9m 11s (- 36m 44s) (1 20%) 1.7474\n",
      "9m 59s (- 39m 58s) (1 20%) 1.7214\n",
      "10m 47s (- 43m 11s) (1 20%) 1.7256\n",
      "11m 36s (- 46m 25s) (1 20%) 1.7061\n",
      "12m 24s (- 49m 39s) (1 20%) 1.6977\n",
      "BLEU Score is 1.5849458255128743\n",
      "13m 21s (- 53m 27s) (1 20%) 1.6440\n",
      "14m 10s (- 56m 41s) (1 20%) 1.6139\n",
      "14m 58s (- 59m 55s) (1 20%) 1.6057\n",
      "15m 47s (- 63m 9s) (1 20%) 1.5497\n",
      "16m 35s (- 66m 23s) (1 20%) 1.5286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score is 3.6044461518555875\n",
      "17m 32s (- 70m 11s) (1 20%) 1.5112\n",
      "18m 21s (- 73m 25s) (1 20%) 1.5094\n",
      "19m 9s (- 76m 39s) (1 20%) 1.5217\n",
      "19m 58s (- 79m 52s) (1 20%) 1.4718\n",
      "20m 46s (- 83m 6s) (1 20%) 1.4746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score is 4.058274958653964\n",
      "21m 43s (- 86m 55s) (1 20%) 1.4567\n",
      "22m 32s (- 90m 8s) (1 20%) 1.4479\n",
      "23m 20s (- 93m 22s) (1 20%) 1.4377\n",
      "24m 9s (- 96m 36s) (1 20%) 1.4359\n",
      "24m 57s (- 99m 51s) (1 20%) 1.4539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score is 4.822015395799312\n",
      "25m 54s (- 103m 39s) (1 20%) 1.4185\n",
      "26m 43s (- 106m 53s) (1 20%) 1.3958\n",
      "27m 32s (- 110m 8s) (1 20%) 1.3960\n",
      "28m 20s (- 113m 22s) (1 20%) 1.4373\n",
      "29m 9s (- 116m 36s) (1 20%) 1.3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score is 4.336333545344074\n",
      "30m 7s (- 120m 30s) (1 20%) 1.3956\n",
      "30m 56s (- 123m 44s) (1 20%) 1.3842\n",
      "31m 44s (- 126m 58s) (1 20%) 1.4073\n",
      "32m 32s (- 130m 11s) (1 20%) 1.3704\n",
      "33m 21s (- 133m 26s) (1 20%) 1.3593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score is 3.9837920082217613\n",
      "34m 19s (- 137m 19s) (1 20%) 1.3509\n",
      "35m 8s (- 140m 33s) (1 20%) 1.3825\n",
      "---------------------------------------------------------------------\n",
      "----------------Saving trained model---------------------------------\n",
      "---------------------------------------------------------------------\n",
      "36m 31s (- 54m 46s) (2 40%) 2.2424\n",
      "37m 19s (- 55m 59s) (2 40%) 1.3412\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "encoder1 = EncoderRNN(EMBEDDING_SIZE,hidden_size).to(device)\n",
    "decoder1 = AttnDecoderRNN(EMBEDDING_SIZE,hidden_size, len(ordered_words_ft_en)).to(device)\n",
    "\n",
    "##UNCOMMENT TO TRAIN THE MODEL\n",
    "trainIters(encoder1, decoder1, 5,'bilstm_att', print_every=50,plot_every = 100, evaluate_every = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search + bleu score\n",
    "def beam_search_decoder(data, k):\n",
    "    sequences = [[list(), 1.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score * -log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        # select top best\n",
    "        sequences = ordered[:1]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_output_node:\n",
    "    def __init__(self,parent, word_idx, prob_sum, isroot=False):\n",
    "        self.parent = parent\n",
    "        self.isroot = isroot\n",
    "        self.children = []\n",
    "        self.word_idx = word_idx\n",
    "        self.prob_sum = prob_sum\n",
    "    \n",
    "    def get_children(self):\n",
    "        '''\n",
    "        return children\n",
    "        '''\n",
    "        return self.children\n",
    "    \n",
    "    def add_children(self, child):\n",
    "        '''\n",
    "        child: node\n",
    "        '''\n",
    "        self.children.append(child)\n",
    "        return\n",
    "    \n",
    "    def get_parent(self):\n",
    "        '''\n",
    "        get parent of children\n",
    "        '''\n",
    "        return self.parent\n",
    "    \n",
    "    def get_word_idx(self):\n",
    "        \n",
    "        return self.word_idx\n",
    "    \n",
    "    def get_prob_sum(self):\n",
    "        \n",
    "        return self.prob_sum\n",
    "    \n",
    "    def is_root(self):\n",
    "        return self.isroot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_sentence_sequence(child_node):\n",
    "    if child_node.is_root():\n",
    "        return [child_node.get_word_idx()]\n",
    "    \n",
    "    return return_sentence_sequence(child_node.get_parent())+[child_node.get_word_idx()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(beam_k, decoder_output, prob_sum = None, parent_node_list=None, vocab_size = len(idx2words_ft_en)):\n",
    "    '''\n",
    "    params:\n",
    "    beam_k\n",
    "    decoder_output: previous round decoder output\n",
    "    parent_node_list: previous candidate word list (for only one candidate)\n",
    "    \n",
    "    return:\n",
    "    list_of_best_k_nodes: best k nodes found in this iteration, list of list, first dim batch, second dim best k\n",
    "    prob_with_sum: probabilistic matrix after sum+sortee \n",
    "    '''\n",
    "    # if first word\n",
    "    if parent_node_list is None:\n",
    "        # initialize result\n",
    "        prob_with_sum_sorted, word_idx_sorted = decoder_output.data.topk(beam_k)\n",
    "        #print(\"ps\",prob_with_sum_sorted)\n",
    "        # add initialize tree list\n",
    "        list_of_best_k_nodes = []\n",
    "        batchsize = prob_with_sum_sorted.shape[0]\n",
    "        for batch_i in range(batchsize):\n",
    "            batch_i_tree_list = []\n",
    "            for beam_i in range(beam_k):\n",
    "                # add tree root node to list\n",
    "                batch_i_tree_list.append(decoder_output_node(parent=None, word_idx=word_idx_sorted[batch_i, beam_i].item(), \n",
    "                                                            prob_sum= prob_with_sum_sorted[batch_i, beam_i].item(), isroot=True))\n",
    "                \n",
    "            list_of_best_k_nodes.append(batch_i_tree_list)\n",
    "   \n",
    "    # if not first word\n",
    "    else:\n",
    "        # get sorted results for all outputs\n",
    "        prob = decoder_output.data\n",
    "        #print(decoder_output.data.shape)\n",
    "        #print(word_idx)\n",
    "        \n",
    "        \n",
    "        # find top beam k words options\n",
    "        #print(\"sum:\",prob_sum)\n",
    "        #print(\"curr prob:\",prob)\n",
    "        #print(\"sum:\",prob+prob_sum)\n",
    "        prob_with_sum = prob+prob_sum\n",
    "        prob_with_sum_sorted, word_idx_sorted = torch.sort(prob_with_sum, dim=1, descending=True)\n",
    "        #print(\"sum sorted:\", prob_with_sum_sorted)\n",
    "        # add top beam k words options into tree\n",
    "        batchsize = prob_with_sum_sorted.shape[0]\n",
    "        \n",
    "        list_of_best_k_nodes = []\n",
    "        for batch_i in range(batchsize):\n",
    "            batch_i_tree_list = []\n",
    "            for beam_i in range(beam_k):\n",
    "                #print(word_idx_sorted[batch_i, beam_i])\n",
    "                #print(parent_node_list[batch_i].get_word_idx())\n",
    "                child_node = decoder_output_node(parent=parent_node_list[batch_i], word_idx= word_idx_sorted[batch_i,beam_i].item(), prob_sum=prob_with_sum_sorted[batch_i,beam_i].item())\n",
    "                \n",
    "                # update parent node's child\n",
    "                parent_node_list[batch_i].add_children(child_node)\n",
    "                #save child to new list\n",
    "                batch_i_tree_list.append(child_node)\n",
    "            # add batch tree list to best k\n",
    "            list_of_best_k_nodes.append(batch_i_tree_list)\n",
    "                \n",
    "    return list_of_best_k_nodes, prob_with_sum_sorted[:,:beam_k], word_idx_sorted[:,:beam_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_beam_search(val_loader,encoder1,decoder1,beam_k = 5):\n",
    "    big_pred_list = []\n",
    "    big_ref_list = []\n",
    "    #beam_k = 5\n",
    "    with torch.no_grad():\n",
    "        predictions = ''\n",
    "        references = ''\n",
    "        for i, (data_s1, data_s2, lengths_s1, lengths_s2) in enumerate(val_loader):\n",
    "            #print(i)\n",
    "            input_tensor = data_s1\n",
    "            input_length = input_tensor.size()[0]\n",
    "            #sentence_length to the output length\n",
    "            sentence_length = data_s2.size()[1]\n",
    "            encoder_hidden = encoder1.initHidden(input_length)\n",
    "\n",
    "            encoder_output, encoder_hidden = encoder1(input_tensor, encoder_hidden)\n",
    "\n",
    "            #decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "            decoder_input = torch.tensor(np.array([[SOS_IDX]]*input_length).reshape(1,input_length),device=device)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoder_attentions = torch.zeros(sentence_length, sentence_length)\n",
    "            decoded_words_eval = []\n",
    "            list_of_best_k_nodes = []\n",
    "\n",
    "            prob_with_sum_sorted = []\n",
    "            #print(\"outside\",prob_with_sum_sorted)\n",
    "\n",
    "            decoder_hidden_list = []\n",
    "            for di in range(sentence_length):\n",
    "\n",
    "                ############################################beam search###################################################\n",
    "                #print(di)\n",
    "                if di == 0:\n",
    "                    decoded_words_sub = []\n",
    "\n",
    "\n",
    "                    decoder_output, decoder_hidden, decoder_attention = decoder1(\n",
    "                                    decoder_input, decoder_hidden, encoder_output)\n",
    "\n",
    "                    # find top k candidates\n",
    "                    list_of_best_k_nodes,prob_with_sum_sorted ,word_idx_sorted = beam_search(beam_k, decoder_output, parent_node_list=None)\n",
    "                    decoder_hidden_list = [decoder_hidden]*beam_k\n",
    "\n",
    "                    #print(\"sum1\",prob_with_sum_sorted)\n",
    "                    #print(\"idx\",word_idx_sorted)\n",
    "                    #print(list_of_best_k_nodes[0][0].get_word_idx())\n",
    "                    #print(list_of_best_k_nodes[0][1].get_word_idx())\n",
    "\n",
    "                else:\n",
    "                    # keep track of all new nodes\n",
    "                    new_nodes = []\n",
    "                    nodes_prob = None\n",
    "                    #nodes_word_idx = None\n",
    "\n",
    "                    # store index in previous candidate to locate position in new nodes, repeats=beam_size*beam_size\n",
    "                    prev_candidate_idx = np.repeat(range(beam_k), repeats=beam_k)\n",
    "\n",
    "                    # iterate through each node candidate from last iterations to find new candidates\n",
    "                    new_decoder_hidden_list = []\n",
    "\n",
    "                    for beam_i in range(beam_k):\n",
    "                        #print(word_idx_sorted.shape)\n",
    "                        topi = word_idx_sorted[:,beam_i].data\n",
    "                        #print(\"idx i\",topi)\n",
    "\n",
    "                        prob_sum = prob_with_sum_sorted[:,beam_i].view((input_length,1))\n",
    "                        #print(\"prob sum:\", prob_sum)\n",
    "                        #change the dimension\n",
    "                        decoder_input = topi.squeeze().detach()\n",
    "                        decoder_input = decoder_input.unsqueeze(0)\n",
    "\n",
    "                        # get decoder output\n",
    "                        decoder_output, decoder_hidden_i, decoder_attention = decoder1(\n",
    "                                    decoder_input, decoder_hidden_list[beam_i], encoder_output)\n",
    "\n",
    "                        new_decoder_hidden_list.append(decoder_hidden_i)\n",
    "\n",
    "                        # get beam search output\n",
    "                        best_k_curr_node, prob_sum_curr_node, _ = beam_search(beam_k, decoder_output, prob_sum=prob_sum, parent_node_list=[ls[beam_i] for ls in list_of_best_k_nodes])\n",
    "                        #print(word_idx_curr_node)\n",
    "\n",
    "                        # keep track of beam search output\n",
    "                        new_nodes.append(best_k_curr_node)\n",
    "\n",
    "                        if beam_i == 0:\n",
    "                            nodes_prob = prob_sum_curr_node.data\n",
    "\n",
    "                            #nodes_word_idx = word_idx_curr_node\n",
    "                        else:\n",
    "                            nodes_prob = torch.cat((nodes_prob, prob_sum_curr_node.data),dim=1)\n",
    "                            #nodes_word_idx = torch.cat((nodes_word_idx, word_idx_curr_node),dim=1)\n",
    "\n",
    "                    #print(\"nodes prob\", nodes_prob)\n",
    "                    _, sorted_idx = torch.sort(nodes_prob, dim=1, descending=True)\n",
    "                    #print(\"length\",nodes_prob.shape)\n",
    "                    #print(nodes_prob)\n",
    "                    #print(sorted_idx)\n",
    "\n",
    "                    #print(prev_candidate_idx)\n",
    "                    #print(\"new nodes len:\", len(new_nodes[0][0]))\n",
    "                    #print(\"new_nodes 0\",new_nodes[0])\n",
    "                    #print(\"new_nodes 1\",new_nodes[1])\n",
    "                    # update \n",
    "                    #print(sorted_idx.shape)\n",
    "                    for batch_i in range(input_length):\n",
    "                        for beam_i in range(beam_k):\n",
    "                            # find the index of which candidate it descended from\n",
    "                            st_idx = sorted_idx[batch_i][beam_i].item()\n",
    "                            # find the corresponding node, st_idx gives parent node id, batch_i gives which example, st_idx%beam_k gives which node in the existing node list\n",
    "                            #if batch_i == 0:\n",
    "                            #print(\"st_idx\",st_idx)\n",
    "                            update_node = new_nodes[prev_candidate_idx[st_idx]][batch_i][st_idx%beam_k]\n",
    "\n",
    "                            list_of_best_k_nodes[batch_i][beam_i] = update_node\n",
    "                            #print(batch_i)\n",
    "                            #print(beam_i)\n",
    "                            #print(list_of_best_k_nodes[0][0].parent.get_word_idx())\n",
    "\n",
    "                            # update word idex, prob sum correspondingly for next iteration\n",
    "                            #word_idx_sorted[batch_i][beam_i] = nodes_word_idx[batch_i][st_idx] \n",
    "                            word_idx_sorted[batch_i][beam_i] = update_node.get_word_idx()\n",
    "                            prob_with_sum_sorted[batch_i][beam_i] = update_node.get_prob_sum()\n",
    "\n",
    "\n",
    "                            decoder_hidden_list[beam_i][0,batch_i,:] = new_decoder_hidden_list[prev_candidate_idx[st_idx]][0,batch_i,:]\n",
    "\n",
    "                    #print(\"best k\",list_of_best_k_nodes[0])\n",
    "                    #print(\"final\", prob_with_sum_sorted)\n",
    "                    #print(\"idx final\", word_idx_sorted)\n",
    "\n",
    "            # find the best and get index\n",
    "            listed_predictions = []\n",
    "            for batch_i in range(input_length):\n",
    "                best_sequence_last_node = list_of_best_k_nodes[batch_i][0]\n",
    "                batch_i_word_idx = return_sentence_sequence(best_sequence_last_node)\n",
    "\n",
    "                listed_predictions.append(' '.join(idx2words_ft_en[token_idx] for token_idx in batch_i_word_idx if token_idx!=PAD_IDX))\n",
    "                #print(' '.join(idx2words_ft_en[token_idx] for token_idx in batch_i_word_idx ))\n",
    "                #print(batch_i_word_idx)\n",
    "                #print (listed_predictions)\n",
    "            listed_reference = []\n",
    "            for ele in data_s2:\n",
    "                sent = index2token_sentence(ele)\n",
    "\n",
    "                listed_reference.append(sent)\n",
    "                #print (\"\\n\")\n",
    "                #print (sent)\n",
    "\n",
    "            #print(listed_predictions)\n",
    "            #bleu_score = corpus_bleu(listed_predictions,[listed_reference])\n",
    "            #print('BLEU Score is %s' % (str(bleu_score.score)))\n",
    "\n",
    "            big_pred_list += listed_predictions\n",
    "            big_ref_list += listed_reference\n",
    "            \n",
    "    bleu_score = corpus_bleu(big_pred_list,[big_ref_list])\n",
    "    print('BLEU Score is %s' % (str(bleu_score.score)))\n",
    "            ############################################beam search###################################################\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
