{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sacrebleu import corpus_bleu\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_load = 100000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "\n",
    "with open('../cc.zh.300.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+3, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    ordered_words_ft.extend(['<pad>', '<unk>', '<s>'])\n",
    "    loaded_embeddings_ft[0,:] = np.zeros(300)\n",
    "    loaded_embeddings_ft[1,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft[2,:] = np.random.normal(size = 300)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+3, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+3\n",
    "        idx2words_ft[i+3] = s[0]\n",
    "        ordered_words_ft.append(s[0])\n",
    "    words_ft['<pad>'] = PAD_IDX\n",
    "    words_ft['<unk>'] = UNK_IDX\n",
    "    words_ft['<s>'] = SOS_IDX\n",
    "    idx2words_ft[PAD_IDX] = '<pad>'\n",
    "    idx2words_ft[UNK_IDX] = '<unk>'\n",
    "    idx2words_ft[SOS_IDX] = '<s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English embedding\n",
    "with open('wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft_en = np.zeros((words_to_load+4, 300))\n",
    "    words_ft_en = {}\n",
    "    idx2words_ft_en = {}\n",
    "    ordered_words_ft_en = []\n",
    "    ordered_words_ft_en.extend(['<pad>', '<unk>', '<s>', '</s>'])\n",
    "    loaded_embeddings_ft_en[0,:] = np.zeros(300)\n",
    "    loaded_embeddings_ft_en[1,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en[2,:] = np.random.normal(size = 300)\n",
    "    loaded_embeddings_ft_en[3,:] = np.random.normal(size = 300)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft_en[i+4, :] = np.asarray(s[1:])\n",
    "        words_ft_en[s[0]] = i+4\n",
    "        idx2words_ft_en[i+4] = s[0]\n",
    "        ordered_words_ft_en.append(s[0])\n",
    "    words_ft_en['<pad>'] = PAD_IDX\n",
    "    words_ft_en['<unk>'] = UNK_IDX\n",
    "    words_ft_en['<s>'] = SOS_IDX\n",
    "    words_ft_en['</s>'] = EOS_IDX\n",
    "    idx2words_ft_en[PAD_IDX] = '<pad>'\n",
    "    idx2words_ft_en[UNK_IDX] = '<unk>'\n",
    "    idx2words_ft_en[SOS_IDX] = '<s>'\n",
    "    idx2words_ft_en[EOS_IDX] = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in chinese-english pairs\n",
    "lines_zh = open('iwslt-zh-en/train.tok.zh',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en = open('iwslt-zh-en/train.tok.en',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_zh_test = open('iwslt-zh-en/test.tok.zh',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_test = open('iwslt-zh-en/test.tok.en',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_zh_val = open('iwslt-zh-en/dev.tok.zh',encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_val = open('iwslt-zh-en/dev.tok.en',encoding = 'utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add sos and eos in each sentence\n",
    "def add_sos_eos(lines):  \n",
    "    train = []\n",
    "    for l in lines:\n",
    "        l = '<s> ' + l + '</s>'\n",
    "        train.append(l)\n",
    "    return train\n",
    "zh_train = add_sos_eos(lines_zh)    \n",
    "en_train = add_sos_eos(lines_en)\n",
    "zh_test = add_sos_eos(lines_zh_test)\n",
    "en_test = add_sos_eos(lines_en_test)\n",
    "zh_val = add_sos_eos(lines_zh_val)\n",
    "en_val = add_sos_eos(lines_en_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data,eng = False):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = []\n",
    "        for token in tokens.split():\n",
    "            if eng == False:\n",
    "                try:\n",
    "                    index_list.append(words_ft[token])\n",
    "                except KeyError:\n",
    "                    index_list.append(UNK_IDX)\n",
    "            else:\n",
    "                try:\n",
    "                    index_list.append(words_ft_en[token])\n",
    "                except KeyError:\n",
    "                    index_list.append(UNK_IDX)\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_train_indices = token2index_dataset(zh_train)\n",
    "en_train_indices = token2index_dataset(en_train,eng = True)\n",
    "zh_test_indices = token2index_dataset(zh_test)\n",
    "en_test_indices = token2index_dataset(en_test,eng = True)\n",
    "zh_val_indices = token2index_dataset(zh_val)\n",
    "en_val_indices = token2index_dataset(en_val,eng = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_sentence_length\n",
    "length_of_en = [len(x.split()) for x in en_train]\n",
    "max_sentence_length_en = 50 #sorted(length_of_en)[-int(len(length_of_en)*0.01)]\n",
    "length_of_zh = [len(x.split()) for x in zh_train]\n",
    "max_sentence_length_zh = 50#sorted(length_of_zh)[-int(len(length_of_zh)*0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(length_of_zh)[-int(len(length_of_zh)*0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Data Loader\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class load_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list_s1,data_list_s2):\n",
    "        \"\"\"\n",
    "        @param data_list_zh: list of Chinese tokens \n",
    "        @param data_list_en: list of English tokens as TARGETS\n",
    "        \"\"\"\n",
    "        self.data_list_s1 = data_list_s1\n",
    "        self.data_list_s2 = data_list_s2\n",
    "        \n",
    "        assert (len(self.data_list_s1) == len(self.data_list_s2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list_s1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx_s1 = self.data_list_s1[key][:max_sentence_length_zh]\n",
    "        token_idx_s2 = self.data_list_s2[key][:max_sentence_length_en]\n",
    "        return [token_idx_s1, token_idx_s2, len(token_idx_s1), len(token_idx_s2)]\n",
    "\n",
    "def collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list_s1 = []\n",
    "    data_list_s2 = []\n",
    "    length_list_s1 = []\n",
    "    length_list_s2 = []\n",
    "    for datum in batch:\n",
    "        length_list_s1.append(datum[2])\n",
    "        length_list_s2.append(datum[3])\n",
    "        padded_vec_zh = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,max_sentence_length_zh-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_en = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,max_sentence_length_en-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_s1.append(padded_vec_zh[:max_sentence_length_zh])\n",
    "        data_list_s2.append(padded_vec_en[:max_sentence_length_en])\n",
    "    #print(type(data_list_s1[0]))\n",
    "    if torch.cuda.is_available and torch.has_cudnn:\n",
    "        return [torch.from_numpy(np.array(data_list_s1)).cuda(), torch.from_numpy(np.array(data_list_s2)).cuda(),\n",
    "                torch.LongTensor(length_list_s1).cuda(), torch.LongTensor(length_list_s2).cuda()]\n",
    "    else:    \n",
    "        return [torch.from_numpy(np.array(data_list_s1)), torch.from_numpy(np.array(data_list_s2)),\n",
    "                torch.LongTensor(length_list_s1), torch.LongTensor(length_list_s2)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 300 # fixed as from the input embedding data\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(zh_train_indices, en_train_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = load_dataset(zh_val_indices, en_val_indices)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_size, embed= torch.from_numpy(loaded_embeddings_ft).float(),num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers \n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed, freeze=True)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_size,num_layers=num_layers,batch_first=True)\n",
    "\n",
    "    def forward(self, data, hidden):\n",
    "        \n",
    "        batch_size, seq_len = data.size()\n",
    "        \n",
    "        embed = self.embedding(data)\n",
    "        \n",
    "        output, hidden = self.gru(embed,hidden)\n",
    "        #hidden = [n layers * n directions =1 , batch_size, hidden_size ]\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    # initialize the hidden with random numbers\n",
    "    def initHidden(self,batch_size):\n",
    "        return torch.randn(self.num_layers, batch_size, self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,emb_dim,hidden_size, output_size, embed= torch.from_numpy(loaded_embeddings_ft_en).float(),num_layers=1,\n",
    "                 dropout_p=0.1, max_length=max_sentence_length_zh):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers \n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed, freeze=True)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.gru = nn.GRU(emb_dim, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, data, hidden,encoder_outputs):\n",
    "        \n",
    "        ### embed: [1 * batch size * emb_dim = 300 ] ###\n",
    "        ### hidden: [1 * batch size * hidden_size = 300 ] ###\n",
    "        ### encoder_outputs: [batch size * max_sentence_length_zh * hidden_size = 300 ] ###\n",
    "        ### 因为这里concat之后，attn layer 他给的是 hidden size *2 \n",
    "        ### 所以我这儿的hidden size就只能写300了 \n",
    "        \n",
    "        embed = self.embedding(data)\n",
    "        embed = self.dropout(embed)    \n",
    "        ### torch.cat((embed, hidden), 2)  \n",
    "        ### [1 * batch size * (emb_dim + hidden_size) ]\n",
    "        \n",
    "        ### attn_weights: [1 * batch size * max_sentence_length_zh ]###\n",
    "        ### attn_weights[0].unsqueeze(1): [batch size * 1 * max_sentence_length_zh ]###\n",
    "        \n",
    "        ### softmax dim=2 因为最后一个dimension是 词组什么的，不能是1，1的话就是\n",
    "        ### 不同batch间这样比较了？\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embed, hidden), 2)), dim=2)\n",
    "        \n",
    "\n",
    "        ### torch.bmm(attn_weights[0].unsqueeze(1),encoder_outputs).squeeze(1) :\n",
    "        ### [batch size * 1 * hidden_size ]###\n",
    "\n",
    "        ### attn_applied: [batch size * hidden_size (= 300) ] ###\n",
    "        attn_applied = torch.bmm(attn_weights[0].unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        ### output: [batch size * hidden_size (= 300) ] ###\n",
    "        ### embed[0]: [batch size * hidden_size (= 300) ] ###\n",
    "\n",
    "        output = torch.cat((embed[0], attn_applied), 1)\n",
    " \n",
    "        ### output: [1 * batch size * hidden_size (= 300) ] ###\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        ### output: [1 * batch size * hidden_size (= 300) ] ###\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return torch.randn(self.num_layers, batch_size, self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "#input_tensor: list of sentence tensor\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "          criterion, eee):\n",
    "    \n",
    "    ### target_tensor [batch size, max_sentence_length_en = 377] ###\n",
    "    ### target_tensor [batch size, max_sentence_length_zh = 220] ###\n",
    "    batch_size_1, input_length = input_tensor.size()\n",
    "    batch_size_2, target_length = target_tensor.size()\n",
    "    \n",
    "    \n",
    "    encoder_hidden = encoder.initHidden(batch_size_1)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "    ### encoder_hidden: 1 * batch * hidden size ### \n",
    "    ### encoder_output: batch size * max_sentence_length_zh * hidden size ### \n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor(np.array([[SOS_IDX]]*batch_size_1).reshape(1,batch_size_1),device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    #print(use_teacher_forcing)\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            \n",
    "            ### decoder_output: [batchsize,5000] ###\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden,encoder_output)\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            decoder_input = target_tensor[:,di].unsqueeze(0)  # Teacher forcing\n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden,encoder_output)\n",
    "                        \n",
    "            ### decoder_output [batch size, 50003]  ###\n",
    "            \n",
    "            ### topi is a [batch size, 1] tensor first we remove the size 1\n",
    "            ### demension then we add it at the beginning using squeeze\n",
    "            ### 有点脑残诶，做个转置不就好了？\n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            \n",
    "            ### decoder_input [1, batch size]  ###\n",
    "            decoder_input = decoder_input.unsqueeze(0)\n",
    " \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "    #--------------------------------------------\t\n",
    "    #\t\n",
    "    #    LOAD MODELS\t\n",
    "    #\t\n",
    "    #--------------------------------------------\t\n",
    "    folder = '.'\t\n",
    "    if not os.path.exists(folder):\t\n",
    "        os.makedirs(folder)\t\n",
    "\n",
    "    if os.path.exists('./attentation_model/Encoder_b'):\t\n",
    "        print('---------------------------------------------------------------------')\t\n",
    "        print('----------------Readind trained model---------------------------------')\t\n",
    "        print('---------------------------------------------------------------------')\t\n",
    "        \t\n",
    "        #read trained models\t\n",
    "        encoder.load_state_dict(torch.load(folder+\"/Encoder_b\"))\n",
    "        decoder.load_state_dict(torch.load(folder+\"/Decoder_b\"))\t\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for i, (data_s1, data_s2, lengths_s1, lengths_s2) in enumerate(train_loader):\n",
    "            input_tensor = data_s1\n",
    "            target_tensor = data_s2\n",
    "            #print(\"train\",target_tensor.size())\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion,i)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                             iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            if i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                \n",
    "        # Save the model for every epoch\n",
    "        print('---------------------------------------------------------------------')\t\n",
    "        print('----------------Saving trained model---------------------------------')\t\n",
    "        print('---------------------------------------------------------------------')\t\n",
    "      \n",
    "        torch.save(encoder.state_dict(),folder +\"/Encoder_b\")\n",
    "        torch.save(decoder.state_dict(),folder +\"/Decoder_b\")\n",
    "\n",
    "    \n",
    "    return plot_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search + bleu score\n",
    "def beam_search_decoder(data, k):\n",
    "    sequences = [[list(), 1.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score * -log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        # select top best\n",
    "        sequences = ordered[:1]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index2token_sentence(sentence_batch):\n",
    "    return ' '.join(idx2words_ft_en[sent.item()] for sent in sentence_batch if sent.item()!=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_output_node:\n",
    "    def __init__(self,parent, word_idx, prob_sum, isroot=False):\n",
    "        self.parent = parent\n",
    "        self.isroot = isroot\n",
    "        self.children = []\n",
    "        self.word_idx = word_idx\n",
    "        self.prob_sum = prob_sum\n",
    "    \n",
    "    def get_children(self):\n",
    "        '''\n",
    "        return children\n",
    "        '''\n",
    "        return self.children\n",
    "    \n",
    "    def add_children(self, child):\n",
    "        '''\n",
    "        child: node\n",
    "        '''\n",
    "        self.children.append(child)\n",
    "        return\n",
    "    \n",
    "    def get_parent(self):\n",
    "        '''\n",
    "        get parent of children\n",
    "        '''\n",
    "        return self.parent\n",
    "    \n",
    "    def get_word_idx(self):\n",
    "        \n",
    "        return self.word_idx\n",
    "    \n",
    "    def get_prob_sum(self):\n",
    "        \n",
    "        return self.prob_sum\n",
    "    \n",
    "    def is_root(self):\n",
    "        return self.isroot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_sentence_sequence(child_node):\n",
    "    if child_node.is_root():\n",
    "        return [child_node.get_word_idx()]\n",
    "    \n",
    "    return return_sentence_sequence(child_node.get_parent())+[child_node.get_word_idx()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(beam_k, decoder_output, prob_sum = None, parent_node_list=None, vocab_size = len(idx2words_ft_en)):\n",
    "    '''\n",
    "    params:\n",
    "    beam_k\n",
    "    decoder_output: previous round decoder output\n",
    "    parent_node_list: previous candidate word list (for only one candidate)\n",
    "    \n",
    "    return:\n",
    "    list_of_best_k_nodes: best k nodes found in this iteration, list of list, first dim batch, second dim best k\n",
    "    prob_with_sum: probabilistic matrix after sum+sortee \n",
    "    '''\n",
    "    # if first word\n",
    "    if parent_node_list is None:\n",
    "        # initialize result\n",
    "        prob_with_sum_sorted, word_idx_sorted = decoder_output.data.topk(beam_k)\n",
    "        #print(\"ps\",prob_with_sum_sorted)\n",
    "        # add initialize tree list\n",
    "        list_of_best_k_nodes = []\n",
    "        batchsize = prob_with_sum_sorted.shape[0]\n",
    "        for batch_i in range(batchsize):\n",
    "            batch_i_tree_list = []\n",
    "            for beam_i in range(beam_k):\n",
    "                # add tree root node to list\n",
    "                batch_i_tree_list.append(decoder_output_node(parent=None, word_idx=word_idx_sorted[batch_i, beam_i].item(), \n",
    "                                                            prob_sum= prob_with_sum_sorted[batch_i, beam_i].item(), isroot=True))\n",
    "                \n",
    "            list_of_best_k_nodes.append(batch_i_tree_list)\n",
    "   \n",
    "    # if not first word\n",
    "    else:\n",
    "        # get sorted results for all outputs\n",
    "        prob = decoder_output.data\n",
    "        #print(decoder_output.data.shape)\n",
    "        #print(word_idx)\n",
    "        \n",
    "        \n",
    "        # find top beam k words options\n",
    "        #print(\"sum:\",prob_sum)\n",
    "        #print(\"curr prob:\",prob)\n",
    "        #print(\"sum:\",prob+prob_sum)\n",
    "        prob_with_sum = prob+prob_sum\n",
    "        prob_with_sum_sorted, word_idx_sorted = torch.sort(prob_with_sum, dim=1, descending=True)\n",
    "        #print(\"sum sorted:\", prob_with_sum_sorted)\n",
    "        # add top beam k words options into tree\n",
    "        batchsize = prob_with_sum_sorted.shape[0]\n",
    "        \n",
    "        list_of_best_k_nodes = []\n",
    "        for batch_i in range(batchsize):\n",
    "            batch_i_tree_list = []\n",
    "            for beam_i in range(beam_k):\n",
    "                #print(word_idx_sorted[batch_i, beam_i])\n",
    "                #print(parent_node_list[batch_i].get_word_idx())\n",
    "                child_node = decoder_output_node(parent=parent_node_list[batch_i], word_idx= word_idx_sorted[batch_i,beam_i].item(), prob_sum=prob_with_sum_sorted[batch_i,beam_i].item())\n",
    "                \n",
    "                # update parent node's child\n",
    "                parent_node_list[batch_i].add_children(child_node)\n",
    "                #save child to new list\n",
    "                batch_i_tree_list.append(child_node)\n",
    "            # add batch tree list to best k\n",
    "            list_of_best_k_nodes.append(batch_i_tree_list)\n",
    "                \n",
    "    return list_of_best_k_nodes, prob_with_sum_sorted[:,:beam_k], word_idx_sorted[:,:beam_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "BLEU Score is 32.73307984141318\n"
     ]
    }
   ],
   "source": [
    "# beam search temp eval\n",
    "beam_k = 5\n",
    "with torch.no_grad():\n",
    "    predictions = ''\n",
    "    references = ''\n",
    "    for i, (data_s1, data_s2, lengths_s1, lengths_s2) in enumerate(val_loader):\n",
    "        print(i)\n",
    "        input_tensor = data_s1\n",
    "        input_length = input_tensor.size()[0]\n",
    "        #sentence_length to the output length\n",
    "        sentence_length = data_s2.size()[1]\n",
    "        encoder_hidden = encoder1.initHidden(input_length)\n",
    "\n",
    "        encoder_output, encoder_hidden = encoder1(input_tensor, encoder_hidden)\n",
    "\n",
    "        #decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_input = torch.tensor(np.array([[SOS_IDX]]*input_length).reshape(1,input_length),device=device)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoder_attentions = torch.zeros(sentence_length, sentence_length)\n",
    "        decoded_words_eval = []\n",
    "        list_of_best_k_nodes = []\n",
    "        \n",
    "        prob_with_sum_sorted = []\n",
    "        #print(\"outside\",prob_with_sum_sorted)\n",
    "        \n",
    "        decoder_hidden_list = []\n",
    "        for di in range(sentence_length):\n",
    "            \n",
    "            ############################################beam search###################################################\n",
    "            #print(di)\n",
    "            if di == 0:\n",
    "                decoded_words_sub = []\n",
    "                \n",
    "                \n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder1(\n",
    "                                decoder_input, decoder_hidden, encoder_output)\n",
    "                \n",
    "                # find top k candidates\n",
    "                list_of_best_k_nodes,prob_with_sum_sorted ,word_idx_sorted = beam_search(beam_k, decoder_output, parent_node_list=None)\n",
    "                decoder_hidden_list = [decoder_hidden]*beam_k\n",
    "                \n",
    "                #print(\"sum1\",prob_with_sum_sorted)\n",
    "                #print(\"idx\",word_idx_sorted)\n",
    "                #print(list_of_best_k_nodes[0][0].get_word_idx())\n",
    "                #print(list_of_best_k_nodes[0][1].get_word_idx())\n",
    "                \n",
    "            else:\n",
    "                # keep track of all new nodes\n",
    "                new_nodes = []\n",
    "                nodes_prob = None\n",
    "                #nodes_word_idx = None\n",
    "                \n",
    "                # store index in previous candidate to locate position in new nodes, repeats=beam_size*beam_size\n",
    "                prev_candidate_idx = np.repeat(range(beam_k), repeats=beam_k)\n",
    "                \n",
    "                # iterate through each node candidate from last iterations to find new candidates\n",
    "                new_decoder_hidden_list = []\n",
    "                \n",
    "                for beam_i in range(beam_k):\n",
    "                    #print(word_idx_sorted.shape)\n",
    "                    topi = word_idx_sorted[:,beam_i].data\n",
    "                    #print(\"idx i\",topi)\n",
    "                    \n",
    "                    prob_sum = prob_with_sum_sorted[:,beam_i].view((input_length,1))\n",
    "                    #print(\"prob sum:\", prob_sum)\n",
    "                    #change the dimension\n",
    "                    decoder_input = topi.squeeze().detach()\n",
    "                    decoder_input = decoder_input.unsqueeze(0)\n",
    "                \n",
    "                    # get decoder output\n",
    "                    decoder_output, decoder_hidden_i, decoder_attention = decoder1(\n",
    "                                decoder_input, decoder_hidden_list[beam_i], encoder_output)\n",
    "                    \n",
    "                    new_decoder_hidden_list.append(decoder_hidden_i)\n",
    "                    \n",
    "                    # get beam search output\n",
    "                    best_k_curr_node, prob_sum_curr_node, _ = beam_search(beam_k, decoder_output, prob_sum=prob_sum, parent_node_list=[ls[beam_i] for ls in list_of_best_k_nodes])\n",
    "                    #print(word_idx_curr_node)\n",
    "                    \n",
    "                    # keep track of beam search output\n",
    "                    new_nodes.append(best_k_curr_node)\n",
    "                    \n",
    "                    if beam_i == 0:\n",
    "                        nodes_prob = prob_sum_curr_node.data\n",
    "                        \n",
    "                        #nodes_word_idx = word_idx_curr_node\n",
    "                    else:\n",
    "                        nodes_prob = torch.cat((nodes_prob, prob_sum_curr_node.data),dim=1)\n",
    "                        #nodes_word_idx = torch.cat((nodes_word_idx, word_idx_curr_node),dim=1)\n",
    "                \n",
    "                #print(\"nodes prob\", nodes_prob)\n",
    "                _, sorted_idx = torch.sort(nodes_prob, dim=1, descending=True)\n",
    "                #print(\"length\",nodes_prob.shape)\n",
    "                #print(nodes_prob)\n",
    "                #print(sorted_idx)\n",
    "                \n",
    "                #print(prev_candidate_idx)\n",
    "                #print(\"new nodes len:\", len(new_nodes[0][0]))\n",
    "                #print(\"new_nodes 0\",new_nodes[0])\n",
    "                #print(\"new_nodes 1\",new_nodes[1])\n",
    "                # update \n",
    "                #print(sorted_idx.shape)\n",
    "                for batch_i in range(input_length):\n",
    "                    for beam_i in range(beam_k):\n",
    "                        # find the index of which candidate it descended from\n",
    "                        st_idx = sorted_idx[batch_i][beam_i].item()\n",
    "                        # find the corresponding node, st_idx gives parent node id, batch_i gives which example, st_idx%beam_k gives which node in the existing node list\n",
    "                        #if batch_i == 0:\n",
    "                        #print(\"st_idx\",st_idx)\n",
    "                        update_node = new_nodes[prev_candidate_idx[st_idx]][batch_i][st_idx%beam_k]\n",
    "                        \n",
    "                        list_of_best_k_nodes[batch_i][beam_i] = update_node\n",
    "                        #print(batch_i)\n",
    "                        #print(beam_i)\n",
    "                        #print(list_of_best_k_nodes[0][0].parent.get_word_idx())\n",
    "                        \n",
    "                        # update word idex, prob sum correspondingly for next iteration\n",
    "                        #word_idx_sorted[batch_i][beam_i] = nodes_word_idx[batch_i][st_idx] \n",
    "                        word_idx_sorted[batch_i][beam_i] = update_node.get_word_idx()\n",
    "                        prob_with_sum_sorted[batch_i][beam_i] = update_node.get_prob_sum()\n",
    "                        \n",
    "                        \n",
    "                        decoder_hidden_list[beam_i][0,batch_i,:] = new_decoder_hidden_list[prev_candidate_idx[st_idx]][0,batch_i,:]\n",
    "                        \n",
    "                #print(\"best k\",list_of_best_k_nodes[0])\n",
    "                #print(\"final\", prob_with_sum_sorted)\n",
    "                #print(\"idx final\", word_idx_sorted)\n",
    "            \n",
    "        # find the best and get index\n",
    "        listed_predictions = []\n",
    "        for batch_i in range(input_length):\n",
    "            best_sequence_last_node = list_of_best_k_nodes[batch_i][0]\n",
    "            batch_i_word_idx = return_sentence_sequence(best_sequence_last_node)\n",
    "            \n",
    "            listed_predictions.append(' '.join(idx2words_ft_en[token_idx] for token_idx in batch_i_word_idx if token_idx!=PAD_IDX))\n",
    "            #print(' '.join(idx2words_ft_en[token_idx] for token_idx in batch_i_word_idx ))\n",
    "            #print(batch_i_word_idx)\n",
    "        listed_reference = []\n",
    "        for ele in data_s2:\n",
    "            sent = index2token_sentence(ele)\n",
    "            \n",
    "            listed_reference.append(sent)\n",
    "            \n",
    "        #print(listed_predictions)\n",
    "        #bleu_score = corpus_bleu(listed_predictions,[listed_reference])\n",
    "        #print('BLEU Score is %s' % (str(bleu_score.score)))\n",
    "        \n",
    "        predictions+= ''.join(listed_predictions)\n",
    "        references += ''.join(listed_reference)\n",
    "bleu_score = corpus_bleu(predictions,references)\n",
    "print('BLEU Score is %s' % (str(bleu_score.score)))\n",
    "        ############################################beam search###################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "BLEU Score is 31.00557185661014\n"
     ]
    }
   ],
   "source": [
    "score_list, output_words, attentions = evaluate(val_loader, encoder1, decoder1, beam=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-62.36996078491211"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sequence_last_node.get_prob_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "#loader can be test_loader or val_loader\n",
    "def evaluate(loader, encoder, decoder, beam = False, beam_k = 1, threshold = 0.5):\n",
    "    bleu_score_list = []\n",
    "    predictions = ''\n",
    "    references = ''\n",
    "    with torch.no_grad():\n",
    "        for i, (data_s1, data_s2, lengths_s1, lengths_s2) in enumerate(loader):\n",
    "            print(i)\n",
    "            input_tensor = data_s1\n",
    "            input_length = input_tensor.size()[0]\n",
    "            #sentence_length to the output length\n",
    "            sentence_length = data_s2.size()[1]\n",
    "            encoder_hidden = encoder.initHidden(input_length)\n",
    "\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "            \n",
    "            #decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "            decoder_input = torch.tensor(np.array([[SOS_IDX]]*input_length).reshape(1,input_length),device=device)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoder_attentions = torch.zeros(sentence_length, sentence_length)\n",
    "            decoded_words_eval = []\n",
    "            # EOS_IDX tensor matrix\n",
    "            test_matrix = torch.ones(input_length, beam_k)*EOS_IDX\n",
    "            out_sequences = []\n",
    "            for di in range(sentence_length):\n",
    "                decoded_words_sub = []\n",
    "                if beam == True:\n",
    "#                     pass\n",
    "                    #input length is the batch_size \n",
    "                    last_word_matrix = torch.zeros([input_length, beam_k])\n",
    "                    # if all last word are the EOS_IDX then break\n",
    "                    if torch.nonzero(last_word_matrix==test_matrix).size(0) >= threshold*input_length*beam_k:\n",
    "                        break\n",
    "                    # initiate\n",
    "                    if di == 0:\n",
    "                        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                            decoder_input, decoder_hidden, encoder_output)\n",
    "                        prob, elements = decoder_output.data.topk(beam_k)\n",
    "                        #batch loop\n",
    "                        for idx, ind in enumerate(elements):\n",
    "                            sequences = []\n",
    "                            for idx2 in range(beam_k):\n",
    "                                # ind[idx2] is the index of vocab\n",
    "                                sequences.append(([ind[idx2].item()], prob[idx][idx2].item()))\n",
    "                                last_word_matrix[idx, idx2] = ind[idx2]\n",
    "                            out_sequences.append(sequences)\n",
    "                    else:\n",
    "                        # shape of decoder output is 1 less than english vocab size, why?\n",
    "                        prob, elements = decoder_output.data.topk(len(idx2words_ft_en)-1)\n",
    "                        #batch loop\n",
    "                        for idx, ind in enumerate(elements):\n",
    "                            #score_list : (1*vocab_size)\n",
    "                            '''\n",
    "                            ? whether score_list make sense\n",
    "                            last_word_matrix every element needs to be tensor\n",
    "                            '''\n",
    "                            score_list = list(prob[idx].cpu())\n",
    "                            updated_dic = {}\n",
    "                            for idx2 in range(beam_k):\n",
    "                                # vocab size list (log)prob + the (log)prob \n",
    "                                updated_score_list = np.array(score_list) + np.array([out_sequences[idx][idx2][1]]*len(score_list))\n",
    "                                #length of vocab_size\n",
    "                                for idx3, ele in enumerate(updated_score_list):\n",
    "                                    # key is the tuple of two indices of vocab # out_sequences[idx][idx2][0]\n",
    "                                    updated_dic[(idx2, idx3)] = ele\n",
    "                            # sort all the dict values and output the keys (tuple of two indices)\n",
    "                            optimal = dict(sorted(updated_dic.items(), key=operator.itemgetter(1), reverse=True)[:beam_k])\n",
    "                            \n",
    "                            #change a dictionary to list of tuples and override it to the out_sequences\n",
    "                            for index, (k, v) in enumerate(optimal.items()):\n",
    "                                (k1, k2) = k\n",
    "                                out_sequences[idx] = [(out_sequences[idx][k1][0].append(k2), v)]\n",
    "                                last_word_matrix[idx][index] = k2\n",
    "                    \n",
    "                    #######################temp##############\n",
    "                    #need to iterate through topi for all choi\n",
    "                    topi = last_word_matrix[:,0]\n",
    "                    #######################temp##############\n",
    "                    # select the first column of out_sequence (which has the hightest softmax value) to do the corpus_blue\n",
    "                    '''\n",
    "                    TO DO\n",
    "                    '''\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                        decoder_input, decoder_hidden, encoder_output)\n",
    "                    # topk(1) - softmax probability maximum\n",
    "                    topv, topi = decoder_output.data.topk(1) \n",
    "                    #print(topi)\n",
    "                    #batch loop\n",
    "                \n",
    "                for ind in topi:\n",
    "                    if ind.item() == EOS_IDX:\n",
    "                        decoded_words_sub.append('</s>')\n",
    "                        break\n",
    "                    else:\n",
    "                        decoded_words_sub.append(idx2words_ft_en[ind.item()])\n",
    "                decoded_words_eval.append(decoded_words_sub)\n",
    "\n",
    "                #change the dimension\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                decoder_input = decoder_input.unsqueeze(0)\n",
    "\n",
    "            pred_num = 0\n",
    "            listed_predictions = []\n",
    "            #swap dimensions of decoded_words to [batch_size * 377]\n",
    "            decoded_words_new = [[i for i in ele] for ele in list(zip(*decoded_words_eval))]\n",
    "           # print(decoded_words_new)\n",
    "            for token_list in decoded_words_new:\n",
    "                sent = ' '.join(token for token in token_list if token!=\"<pad>\")\n",
    "                #print(len(token_list))\n",
    "                #print (sent)\n",
    "                listed_predictions.append(sent)\n",
    "                pred_num += 1\n",
    "                \n",
    "            ref_num = 0\n",
    "            listed_reference = []\n",
    "            for ele in data_s2:\n",
    "                sent = index2token_sentence(ele)\n",
    "                #print (tokens)\n",
    "                #sent = ' '.join(tokens)\n",
    "                #print (sent)\n",
    "                listed_reference.append(sent)\n",
    "                ref_num += 1\n",
    "            #print(listed_reference)\n",
    "            predictions+= ''.join(listed_predictions)\n",
    "            references += ''.join(listed_reference)\n",
    "            #bleu_score = corpus_bleu(listed_predictions,[listed_reference])\n",
    "            #print('BLEU Score is %s' % (str(bleu_score.score)))\n",
    "        #bleu_score_list.append(bleu_score)\n",
    "        bleu_score = corpus_bleu(predictions,references)\n",
    "        print('BLEU Score is %s' % (str(bleu_score.score)))\n",
    "        return bleu_score_list, decoded_words_new, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "encoder1 = EncoderRNN(EMBEDDING_SIZE,hidden_size).to(device)\n",
    "decoder1 = AttnDecoderRNN(EMBEDDING_SIZE,hidden_size, len(ordered_words_ft)).to(device)\n",
    "\n",
    "# ##UNCOMMENT TO TRAIN THE MODEL\n",
    "#trainIters(encoder1, decoder1, 1, print_every=50)\n",
    "encoder1.load_state_dict(torch.load(\"encoder.pt\"))\n",
    "decoder1.load_state_dict(torch.load(\"decoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "BLEU Score is 17.71420876141106\n",
      "1\n",
      "BLEU Score is 12.316699475775277\n",
      "2\n",
      "BLEU Score is 16.771978844457923\n",
      "3\n",
      "BLEU Score is 14.820103268785735\n",
      "4\n",
      "BLEU Score is 15.020796067730476\n",
      "5\n",
      "BLEU Score is 14.604746844090613\n",
      "6\n",
      "BLEU Score is 15.200883631573625\n",
      "7\n",
      "BLEU Score is 19.642487766861198\n",
      "8\n",
      "BLEU Score is 12.639170533914779\n",
      "9\n",
      "BLEU Score is 17.06120443640888\n",
      "10\n",
      "BLEU Score is 15.335374224015915\n",
      "11\n",
      "BLEU Score is 12.58010525856447\n",
      "12\n",
      "BLEU Score is 12.503701041814471\n",
      "13\n",
      "BLEU Score is 15.204916592863635\n",
      "14\n",
      "BLEU Score is 12.965521301040711\n",
      "15\n",
      "BLEU Score is 15.6283631068406\n",
      "16\n",
      "BLEU Score is 15.29426993596645\n",
      "17\n",
      "BLEU Score is 19.106286196964813\n",
      "18\n",
      "BLEU Score is 17.35800934690769\n",
      "19\n",
      "BLEU Score is 17.017165541121212\n",
      "20\n",
      "BLEU Score is 11.215424667921816\n",
      "21\n",
      "BLEU Score is 9.994893917155238\n",
      "22\n",
      "BLEU Score is 15.142366652672127\n",
      "23\n",
      "BLEU Score is 9.515641418436516\n",
      "24\n",
      "BLEU Score is 13.31368016458013\n",
      "25\n",
      "BLEU Score is 16.486262314765632\n",
      "26\n",
      "BLEU Score is 15.256161281269046\n",
      "27\n",
      "BLEU Score is 13.537504544690199\n",
      "28\n",
      "BLEU Score is 16.903270937516062\n",
      "29\n",
      "BLEU Score is 13.220958817830581\n",
      "30\n",
      "BLEU Score is 14.71227476694999\n",
      "31\n",
      "BLEU Score is 15.495536173781302\n",
      "32\n",
      "BLEU Score is 12.473119442357442\n",
      "33\n",
      "BLEU Score is 11.180546885336296\n",
      "34\n",
      "BLEU Score is 12.62001409151804\n",
      "35\n",
      "BLEU Score is 12.453810289014774\n",
      "36\n",
      "BLEU Score is 10.606765036478029\n",
      "37\n",
      "BLEU Score is 16.724144402118856\n",
      "38\n",
      "BLEU Score is 15.54459648438355\n",
      "39\n",
      "BLEU Score is 14.651632186488964\n"
     ]
    }
   ],
   "source": [
    "score_list, output_words, attentions = evaluate(val_loader, encoder1, decoder1, beam=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
