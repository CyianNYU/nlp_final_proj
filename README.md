# nlp_final_proj

For nlp final project
- gru_no_attention.ipynb: Seq2Seq model with GRU RNN model without attention
- bidirection_gru.ipynb: bidirectional GRU RNN model with luong attention
- bidirection_lstm.ipynb: bidirectional LSTM RNN model with luong attention, evaluation, beam search evaluation, dataloader
- cnn.ipynb: Seq2Seq model with CNN encoder
- self_attention.ipynb: self attention
- Archive: historical codes

# Contributers:
- Tianyi Bi
- Yiyan Chen
- Manwen Li
- Tinghao Li

# Reference:
- https://stackoverflow.com/questions/50571991/implementing-luong-attention-in-pytorch
- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
- http://nlp.seas.harvard.edu/2018/04/03/attention.html

